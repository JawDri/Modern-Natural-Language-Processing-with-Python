{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_for_NLP_udemy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9JJ7FBw84tG",
        "colab_type": "text"
      },
      "source": [
        "# Stage 1: Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcvtPlp3YWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUaxfAAABtZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget https://www.statmt.org/europarl/v7/fr-en.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_hdxyjzB4F3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tar -xf fr-en.tgz.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6o_cpZz3y_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQN8jwx48_yU",
        "colab_type": "text"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlOT-2mlw0r",
        "colab_type": "text"
      },
      "source": [
        "## Loading files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCD9jwXsLwS_",
        "colab_type": "text"
      },
      "source": [
        "We import files from our personal google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpbl1pXCR0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Or0sLV5b8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "with open(\"/content/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/nonbreaking_prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFw0D2vP_Dl",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIBeGXn7LIJ",
        "colab_type": "text"
      },
      "source": [
        "Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_TeuktU40Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9x4mZfKMaxD",
        "colab_type": "text"
      },
      "source": [
        "We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-8LLK-WdFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Y9v8-Tozl2",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YXanmOd_xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftIbPzIwCtwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPFe2YJDC9jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG6AlcFMpC5C",
        "colab_type": "text"
      },
      "source": [
        "## Remove too long sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CD6PLGyQWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypm8h5aZQTZ1",
        "colab_type": "text"
      },
      "source": [
        "## Inputs/outputs creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FP0WPsdM8hl",
        "colab_type": "text"
      },
      "source": [
        "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDfLDWUONlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxMp3TOIYff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycT0YqydRcUd",
        "colab_type": "text"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SBoH8G4XyR9",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2wc6sYlX0dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcw8YIQqRhOJ",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sffhwwvX-wj",
        "colab_type": "text"
      },
      "source": [
        "### Attention computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rEoCNJURbrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MjtvXrfYEx7",
        "colab_type": "text"
      },
      "source": [
        "### Multi-head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvq4I9uTX5p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiyuHe1OeT5N",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV0ZMH7KT_KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-P92KeZih60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DthraBEwuvl",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZWZyFBnwy8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpzdiWHiwywF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5sJYkjbz5DD",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqvqNjJPwyh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-LRThUPrso",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOdqQ5qPs8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xg4Wrg1Wgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Goque362343",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb_32PIU5Zkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/projects/transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFK5kUx602K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c59aa2b-7d3b-4ce7-965b-31ac79920262"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.4120 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.2772 Accuracy 0.0070\n",
            "Epoch 1 Batch 100 Loss 6.2022 Accuracy 0.0295\n",
            "Epoch 1 Batch 150 Loss 6.1356 Accuracy 0.0372\n",
            "Epoch 1 Batch 200 Loss 6.0428 Accuracy 0.0410\n",
            "Epoch 1 Batch 250 Loss 5.9341 Accuracy 0.0433\n",
            "Epoch 1 Batch 300 Loss 5.8029 Accuracy 0.0461\n",
            "Epoch 1 Batch 350 Loss 5.6693 Accuracy 0.0521\n",
            "Epoch 1 Batch 400 Loss 5.5371 Accuracy 0.0578\n",
            "Epoch 1 Batch 450 Loss 5.4213 Accuracy 0.0630\n",
            "Epoch 1 Batch 500 Loss 5.3145 Accuracy 0.0684\n",
            "Epoch 1 Batch 550 Loss 5.2143 Accuracy 0.0739\n",
            "Epoch 1 Batch 600 Loss 5.1158 Accuracy 0.0794\n",
            "Epoch 1 Batch 650 Loss 5.0235 Accuracy 0.0850\n",
            "Epoch 1 Batch 700 Loss 4.9359 Accuracy 0.0904\n",
            "Epoch 1 Batch 750 Loss 4.8500 Accuracy 0.0956\n",
            "Epoch 1 Batch 800 Loss 4.7689 Accuracy 0.1006\n",
            "Epoch 1 Batch 850 Loss 4.6908 Accuracy 0.1058\n",
            "Epoch 1 Batch 900 Loss 4.6179 Accuracy 0.1107\n",
            "Epoch 1 Batch 950 Loss 4.5481 Accuracy 0.1152\n",
            "Epoch 1 Batch 1000 Loss 4.4832 Accuracy 0.1197\n",
            "Epoch 1 Batch 1050 Loss 4.4224 Accuracy 0.1240\n",
            "Epoch 1 Batch 1100 Loss 4.3667 Accuracy 0.1277\n",
            "Epoch 1 Batch 1150 Loss 4.3145 Accuracy 0.1313\n",
            "Epoch 1 Batch 1200 Loss 4.2620 Accuracy 0.1345\n",
            "Epoch 1 Batch 1250 Loss 4.2126 Accuracy 0.1378\n",
            "Epoch 1 Batch 1300 Loss 4.1672 Accuracy 0.1410\n",
            "Epoch 1 Batch 1350 Loss 4.1245 Accuracy 0.1440\n",
            "Epoch 1 Batch 1400 Loss 4.0818 Accuracy 0.1472\n",
            "Epoch 1 Batch 1450 Loss 4.0411 Accuracy 0.1502\n",
            "Epoch 1 Batch 1500 Loss 4.0017 Accuracy 0.1533\n",
            "Epoch 1 Batch 1550 Loss 3.9643 Accuracy 0.1563\n",
            "Epoch 1 Batch 1600 Loss 3.9288 Accuracy 0.1592\n",
            "Epoch 1 Batch 1650 Loss 3.8950 Accuracy 0.1620\n",
            "Epoch 1 Batch 1700 Loss 3.8618 Accuracy 0.1647\n",
            "Epoch 1 Batch 1750 Loss 3.8309 Accuracy 0.1674\n",
            "Epoch 1 Batch 1800 Loss 3.7998 Accuracy 0.1700\n",
            "Epoch 1 Batch 1850 Loss 3.7706 Accuracy 0.1725\n",
            "Epoch 1 Batch 1900 Loss 3.7414 Accuracy 0.1749\n",
            "Epoch 1 Batch 1950 Loss 3.7139 Accuracy 0.1774\n",
            "Epoch 1 Batch 2000 Loss 3.6867 Accuracy 0.1797\n",
            "Epoch 1 Batch 2050 Loss 3.6606 Accuracy 0.1818\n",
            "Epoch 1 Batch 2100 Loss 3.6349 Accuracy 0.1838\n",
            "Epoch 1 Batch 2150 Loss 3.6086 Accuracy 0.1857\n",
            "Epoch 1 Batch 2200 Loss 3.5819 Accuracy 0.1875\n",
            "Epoch 1 Batch 2250 Loss 3.5555 Accuracy 0.1895\n",
            "Epoch 1 Batch 2300 Loss 3.5299 Accuracy 0.1913\n",
            "Epoch 1 Batch 2350 Loss 3.5052 Accuracy 0.1932\n",
            "Epoch 1 Batch 2400 Loss 3.4811 Accuracy 0.1952\n",
            "Epoch 1 Batch 2450 Loss 3.4564 Accuracy 0.1971\n",
            "Epoch 1 Batch 2500 Loss 3.4327 Accuracy 0.1991\n",
            "Epoch 1 Batch 2550 Loss 3.4089 Accuracy 0.2010\n",
            "Epoch 1 Batch 2600 Loss 3.3870 Accuracy 0.2029\n",
            "Epoch 1 Batch 2650 Loss 3.3655 Accuracy 0.2047\n",
            "Epoch 1 Batch 2700 Loss 3.3441 Accuracy 0.2066\n",
            "Epoch 1 Batch 2750 Loss 3.3236 Accuracy 0.2085\n",
            "Epoch 1 Batch 2800 Loss 3.3028 Accuracy 0.2103\n",
            "Epoch 1 Batch 2850 Loss 3.2828 Accuracy 0.2122\n",
            "Epoch 1 Batch 2900 Loss 3.2628 Accuracy 0.2141\n",
            "Epoch 1 Batch 2950 Loss 3.2428 Accuracy 0.2159\n",
            "Epoch 1 Batch 3000 Loss 3.2237 Accuracy 0.2177\n",
            "Epoch 1 Batch 3050 Loss 3.2045 Accuracy 0.2194\n",
            "Epoch 1 Batch 3100 Loss 3.1855 Accuracy 0.2212\n",
            "Epoch 1 Batch 3150 Loss 3.1669 Accuracy 0.2230\n",
            "Epoch 1 Batch 3200 Loss 3.1484 Accuracy 0.2248\n",
            "Epoch 1 Batch 3250 Loss 3.1302 Accuracy 0.2266\n",
            "Epoch 1 Batch 3300 Loss 3.1121 Accuracy 0.2285\n",
            "Epoch 1 Batch 3350 Loss 3.0947 Accuracy 0.2302\n",
            "Epoch 1 Batch 3400 Loss 3.0776 Accuracy 0.2320\n",
            "Epoch 1 Batch 3450 Loss 3.0602 Accuracy 0.2337\n",
            "Epoch 1 Batch 3500 Loss 3.0439 Accuracy 0.2355\n",
            "Epoch 1 Batch 3550 Loss 3.0274 Accuracy 0.2373\n",
            "Epoch 1 Batch 3600 Loss 3.0108 Accuracy 0.2391\n",
            "Epoch 1 Batch 3650 Loss 2.9949 Accuracy 0.2408\n",
            "Epoch 1 Batch 3700 Loss 2.9795 Accuracy 0.2425\n",
            "Epoch 1 Batch 3750 Loss 2.9644 Accuracy 0.2443\n",
            "Epoch 1 Batch 3800 Loss 2.9494 Accuracy 0.2460\n",
            "Epoch 1 Batch 3850 Loss 2.9346 Accuracy 0.2477\n",
            "Epoch 1 Batch 3900 Loss 2.9203 Accuracy 0.2493\n",
            "Epoch 1 Batch 3950 Loss 2.9061 Accuracy 0.2510\n",
            "Epoch 1 Batch 4000 Loss 2.8914 Accuracy 0.2527\n",
            "Epoch 1 Batch 4050 Loss 2.8775 Accuracy 0.2543\n",
            "Epoch 1 Batch 4100 Loss 2.8637 Accuracy 0.2558\n",
            "Epoch 1 Batch 4150 Loss 2.8510 Accuracy 0.2573\n",
            "Epoch 1 Batch 4200 Loss 2.8392 Accuracy 0.2586\n",
            "Epoch 1 Batch 4250 Loss 2.8278 Accuracy 0.2599\n",
            "Epoch 1 Batch 4300 Loss 2.8164 Accuracy 0.2611\n",
            "Epoch 1 Batch 4350 Loss 2.8058 Accuracy 0.2623\n",
            "Epoch 1 Batch 4400 Loss 2.7953 Accuracy 0.2635\n",
            "Epoch 1 Batch 4450 Loss 2.7847 Accuracy 0.2646\n",
            "Epoch 1 Batch 4500 Loss 2.7744 Accuracy 0.2657\n",
            "Epoch 1 Batch 4550 Loss 2.7642 Accuracy 0.2668\n",
            "Epoch 1 Batch 4600 Loss 2.7541 Accuracy 0.2678\n",
            "Epoch 1 Batch 4650 Loss 2.7440 Accuracy 0.2689\n",
            "Epoch 1 Batch 4700 Loss 2.7347 Accuracy 0.2700\n",
            "Epoch 1 Batch 4750 Loss 2.7252 Accuracy 0.2710\n",
            "Epoch 1 Batch 4800 Loss 2.7154 Accuracy 0.2721\n",
            "Epoch 1 Batch 4850 Loss 2.7056 Accuracy 0.2732\n",
            "Epoch 1 Batch 4900 Loss 2.6964 Accuracy 0.2742\n",
            "Epoch 1 Batch 4950 Loss 2.6873 Accuracy 0.2752\n",
            "Epoch 1 Batch 5000 Loss 2.6783 Accuracy 0.2762\n",
            "Epoch 1 Batch 5050 Loss 2.6691 Accuracy 0.2772\n",
            "Epoch 1 Batch 5100 Loss 2.6601 Accuracy 0.2781\n",
            "Epoch 1 Batch 5150 Loss 2.6511 Accuracy 0.2790\n",
            "Epoch 1 Batch 5200 Loss 2.6423 Accuracy 0.2799\n",
            "Epoch 1 Batch 5250 Loss 2.6339 Accuracy 0.2808\n",
            "Epoch 1 Batch 5300 Loss 2.6253 Accuracy 0.2817\n",
            "Epoch 1 Batch 5350 Loss 2.6172 Accuracy 0.2825\n",
            "Epoch 1 Batch 5400 Loss 2.6089 Accuracy 0.2834\n",
            "Epoch 1 Batch 5450 Loss 2.6005 Accuracy 0.2842\n",
            "Epoch 1 Batch 5500 Loss 2.5921 Accuracy 0.2851\n",
            "Epoch 1 Batch 5550 Loss 2.5841 Accuracy 0.2859\n",
            "Epoch 1 Batch 5600 Loss 2.5758 Accuracy 0.2867\n",
            "Epoch 1 Batch 5650 Loss 2.5678 Accuracy 0.2875\n",
            "Epoch 1 Batch 5700 Loss 2.5600 Accuracy 0.2883\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/projects/transformer/ckpt/ckpt-1\n",
            "Time taken for 1 epoch: 2216.064268350601 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.6304 Accuracy 0.3947\n",
            "Epoch 2 Batch 50 Loss 1.6753 Accuracy 0.3853\n",
            "Epoch 2 Batch 100 Loss 1.6888 Accuracy 0.3842\n",
            "Epoch 2 Batch 150 Loss 1.6750 Accuracy 0.3856\n",
            "Epoch 2 Batch 200 Loss 1.6710 Accuracy 0.3866\n",
            "Epoch 2 Batch 250 Loss 1.6681 Accuracy 0.3878\n",
            "Epoch 2 Batch 300 Loss 1.6601 Accuracy 0.3893\n",
            "Epoch 2 Batch 350 Loss 1.6584 Accuracy 0.3903\n",
            "Epoch 2 Batch 400 Loss 1.6534 Accuracy 0.3910\n",
            "Epoch 2 Batch 450 Loss 1.6457 Accuracy 0.3918\n",
            "Epoch 2 Batch 500 Loss 1.6409 Accuracy 0.3921\n",
            "Epoch 2 Batch 550 Loss 1.6352 Accuracy 0.3924\n",
            "Epoch 2 Batch 600 Loss 1.6324 Accuracy 0.3929\n",
            "Epoch 2 Batch 650 Loss 1.6288 Accuracy 0.3932\n",
            "Epoch 2 Batch 700 Loss 1.6265 Accuracy 0.3939\n",
            "Epoch 2 Batch 750 Loss 1.6224 Accuracy 0.3944\n",
            "Epoch 2 Batch 800 Loss 1.6210 Accuracy 0.3949\n",
            "Epoch 2 Batch 850 Loss 1.6164 Accuracy 0.3952\n",
            "Epoch 2 Batch 900 Loss 1.6139 Accuracy 0.3954\n",
            "Epoch 2 Batch 950 Loss 1.6089 Accuracy 0.3959\n",
            "Epoch 2 Batch 1000 Loss 1.6049 Accuracy 0.3963\n",
            "Epoch 2 Batch 1050 Loss 1.6017 Accuracy 0.3965\n",
            "Epoch 2 Batch 1100 Loss 1.5989 Accuracy 0.3969\n",
            "Epoch 2 Batch 1150 Loss 1.5948 Accuracy 0.3973\n",
            "Epoch 2 Batch 1200 Loss 1.5915 Accuracy 0.3976\n",
            "Epoch 2 Batch 1250 Loss 1.5887 Accuracy 0.3982\n",
            "Epoch 2 Batch 1300 Loss 1.5852 Accuracy 0.3988\n",
            "Epoch 2 Batch 1350 Loss 1.5811 Accuracy 0.3994\n",
            "Epoch 2 Batch 1400 Loss 1.5772 Accuracy 0.4001\n",
            "Epoch 2 Batch 1450 Loss 1.5736 Accuracy 0.4010\n",
            "Epoch 2 Batch 1500 Loss 1.5695 Accuracy 0.4019\n",
            "Epoch 2 Batch 1550 Loss 1.5655 Accuracy 0.4030\n",
            "Epoch 2 Batch 1600 Loss 1.5616 Accuracy 0.4040\n",
            "Epoch 2 Batch 1650 Loss 1.5586 Accuracy 0.4047\n",
            "Epoch 2 Batch 1700 Loss 1.5543 Accuracy 0.4056\n",
            "Epoch 2 Batch 1750 Loss 1.5507 Accuracy 0.4063\n",
            "Epoch 2 Batch 1800 Loss 1.5470 Accuracy 0.4073\n",
            "Epoch 2 Batch 1850 Loss 1.5435 Accuracy 0.4084\n",
            "Epoch 2 Batch 1900 Loss 1.5392 Accuracy 0.4094\n",
            "Epoch 2 Batch 1950 Loss 1.5359 Accuracy 0.4102\n",
            "Epoch 2 Batch 2000 Loss 1.5321 Accuracy 0.4110\n",
            "Epoch 2 Batch 2050 Loss 1.5286 Accuracy 0.4118\n",
            "Epoch 2 Batch 2100 Loss 1.5241 Accuracy 0.4124\n",
            "Epoch 2 Batch 2150 Loss 1.5199 Accuracy 0.4128\n",
            "Epoch 2 Batch 2200 Loss 1.5152 Accuracy 0.4134\n",
            "Epoch 2 Batch 2250 Loss 1.5104 Accuracy 0.4138\n",
            "Epoch 2 Batch 2300 Loss 1.5048 Accuracy 0.4143\n",
            "Epoch 2 Batch 2350 Loss 1.5000 Accuracy 0.4148\n",
            "Epoch 2 Batch 2400 Loss 1.4956 Accuracy 0.4153\n",
            "Epoch 2 Batch 2450 Loss 1.4909 Accuracy 0.4158\n",
            "Epoch 2 Batch 2500 Loss 1.4867 Accuracy 0.4165\n",
            "Epoch 2 Batch 2550 Loss 1.4823 Accuracy 0.4171\n",
            "Epoch 2 Batch 2600 Loss 1.4777 Accuracy 0.4176\n",
            "Epoch 2 Batch 2650 Loss 1.4729 Accuracy 0.4183\n",
            "Epoch 2 Batch 2700 Loss 1.4684 Accuracy 0.4189\n",
            "Epoch 2 Batch 2750 Loss 1.4647 Accuracy 0.4195\n",
            "Epoch 2 Batch 2800 Loss 1.4611 Accuracy 0.4201\n",
            "Epoch 2 Batch 2850 Loss 1.4569 Accuracy 0.4206\n",
            "Epoch 2 Batch 2900 Loss 1.4532 Accuracy 0.4211\n",
            "Epoch 2 Batch 2950 Loss 1.4493 Accuracy 0.4216\n",
            "Epoch 2 Batch 3000 Loss 1.4457 Accuracy 0.4221\n",
            "Epoch 2 Batch 3050 Loss 1.4422 Accuracy 0.4227\n",
            "Epoch 2 Batch 3100 Loss 1.4382 Accuracy 0.4232\n",
            "Epoch 2 Batch 3150 Loss 1.4345 Accuracy 0.4237\n",
            "Epoch 2 Batch 3200 Loss 1.4309 Accuracy 0.4241\n",
            "Epoch 2 Batch 3250 Loss 1.4269 Accuracy 0.4247\n",
            "Epoch 2 Batch 3300 Loss 1.4234 Accuracy 0.4252\n",
            "Epoch 2 Batch 3350 Loss 1.4193 Accuracy 0.4258\n",
            "Epoch 2 Batch 3400 Loss 1.4158 Accuracy 0.4264\n",
            "Epoch 2 Batch 3450 Loss 1.4124 Accuracy 0.4269\n",
            "Epoch 2 Batch 3500 Loss 1.4089 Accuracy 0.4275\n",
            "Epoch 2 Batch 3550 Loss 1.4056 Accuracy 0.4279\n",
            "Epoch 2 Batch 3600 Loss 1.4021 Accuracy 0.4284\n",
            "Epoch 2 Batch 3650 Loss 1.3987 Accuracy 0.4289\n",
            "Epoch 2 Batch 3700 Loss 1.3955 Accuracy 0.4295\n",
            "Epoch 2 Batch 3750 Loss 1.3922 Accuracy 0.4301\n",
            "Epoch 2 Batch 3800 Loss 1.3891 Accuracy 0.4306\n",
            "Epoch 2 Batch 3850 Loss 1.3863 Accuracy 0.4311\n",
            "Epoch 2 Batch 3900 Loss 1.3834 Accuracy 0.4317\n",
            "Epoch 2 Batch 3950 Loss 1.3803 Accuracy 0.4322\n",
            "Epoch 2 Batch 4000 Loss 1.3773 Accuracy 0.4327\n",
            "Epoch 2 Batch 4050 Loss 1.3745 Accuracy 0.4333\n",
            "Epoch 2 Batch 4100 Loss 1.3720 Accuracy 0.4338\n",
            "Epoch 2 Batch 4150 Loss 1.3702 Accuracy 0.4340\n",
            "Epoch 2 Batch 4200 Loss 1.3691 Accuracy 0.4343\n",
            "Epoch 2 Batch 4250 Loss 1.3682 Accuracy 0.4345\n",
            "Epoch 2 Batch 4300 Loss 1.3672 Accuracy 0.4346\n",
            "Epoch 2 Batch 4350 Loss 1.3671 Accuracy 0.4347\n",
            "Epoch 2 Batch 4400 Loss 1.3664 Accuracy 0.4348\n",
            "Epoch 2 Batch 4450 Loss 1.3662 Accuracy 0.4348\n",
            "Epoch 2 Batch 4500 Loss 1.3661 Accuracy 0.4348\n",
            "Epoch 2 Batch 4550 Loss 1.3661 Accuracy 0.4348\n",
            "Epoch 2 Batch 4600 Loss 1.3660 Accuracy 0.4348\n",
            "Epoch 2 Batch 4650 Loss 1.3659 Accuracy 0.4348\n",
            "Epoch 2 Batch 4700 Loss 1.3659 Accuracy 0.4348\n",
            "Epoch 2 Batch 4750 Loss 1.3658 Accuracy 0.4349\n",
            "Epoch 2 Batch 4800 Loss 1.3659 Accuracy 0.4349\n",
            "Epoch 2 Batch 4850 Loss 1.3658 Accuracy 0.4349\n",
            "Epoch 2 Batch 4900 Loss 1.3658 Accuracy 0.4348\n",
            "Epoch 2 Batch 4950 Loss 1.3659 Accuracy 0.4348\n",
            "Epoch 2 Batch 5000 Loss 1.3658 Accuracy 0.4348\n",
            "Epoch 2 Batch 5050 Loss 1.3659 Accuracy 0.4348\n",
            "Epoch 2 Batch 5100 Loss 1.3660 Accuracy 0.4348\n",
            "Epoch 2 Batch 5150 Loss 1.3662 Accuracy 0.4347\n",
            "Epoch 2 Batch 5200 Loss 1.3661 Accuracy 0.4347\n",
            "Epoch 2 Batch 5250 Loss 1.3660 Accuracy 0.4346\n",
            "Epoch 2 Batch 5300 Loss 1.3658 Accuracy 0.4345\n",
            "Epoch 2 Batch 5350 Loss 1.3659 Accuracy 0.4344\n",
            "Epoch 2 Batch 5400 Loss 1.3658 Accuracy 0.4343\n",
            "Epoch 2 Batch 5450 Loss 1.3656 Accuracy 0.4343\n",
            "Epoch 2 Batch 5500 Loss 1.3655 Accuracy 0.4342\n",
            "Epoch 2 Batch 5550 Loss 1.3653 Accuracy 0.4342\n",
            "Epoch 2 Batch 5600 Loss 1.3648 Accuracy 0.4341\n",
            "Epoch 2 Batch 5650 Loss 1.3642 Accuracy 0.4342\n",
            "Epoch 2 Batch 5700 Loss 1.3641 Accuracy 0.4341\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/projects/transformer/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 2087.735672712326 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.5338 Accuracy 0.4071\n",
            "Epoch 3 Batch 50 Loss 1.3626 Accuracy 0.4347\n",
            "Epoch 3 Batch 100 Loss 1.3384 Accuracy 0.4398\n",
            "Epoch 3 Batch 150 Loss 1.3287 Accuracy 0.4391\n",
            "Epoch 3 Batch 200 Loss 1.3287 Accuracy 0.4387\n",
            "Epoch 3 Batch 250 Loss 1.3194 Accuracy 0.4385\n",
            "Epoch 3 Batch 300 Loss 1.3208 Accuracy 0.4392\n",
            "Epoch 3 Batch 350 Loss 1.3236 Accuracy 0.4392\n",
            "Epoch 3 Batch 400 Loss 1.3198 Accuracy 0.4392\n",
            "Epoch 3 Batch 450 Loss 1.3160 Accuracy 0.4392\n",
            "Epoch 3 Batch 500 Loss 1.3140 Accuracy 0.4395\n",
            "Epoch 3 Batch 550 Loss 1.3101 Accuracy 0.4393\n",
            "Epoch 3 Batch 600 Loss 1.3111 Accuracy 0.4394\n",
            "Epoch 3 Batch 650 Loss 1.3112 Accuracy 0.4395\n",
            "Epoch 3 Batch 700 Loss 1.3110 Accuracy 0.4395\n",
            "Epoch 3 Batch 750 Loss 1.3101 Accuracy 0.4396\n",
            "Epoch 3 Batch 800 Loss 1.3067 Accuracy 0.4401\n",
            "Epoch 3 Batch 850 Loss 1.3044 Accuracy 0.4403\n",
            "Epoch 3 Batch 900 Loss 1.3039 Accuracy 0.4404\n",
            "Epoch 3 Batch 950 Loss 1.3031 Accuracy 0.4405\n",
            "Epoch 3 Batch 1000 Loss 1.3005 Accuracy 0.4409\n",
            "Epoch 3 Batch 1050 Loss 1.2993 Accuracy 0.4410\n",
            "Epoch 3 Batch 1100 Loss 1.2974 Accuracy 0.4411\n",
            "Epoch 3 Batch 1150 Loss 1.2954 Accuracy 0.4412\n",
            "Epoch 3 Batch 1200 Loss 1.2924 Accuracy 0.4416\n",
            "Epoch 3 Batch 1250 Loss 1.2902 Accuracy 0.4421\n",
            "Epoch 3 Batch 1300 Loss 1.2880 Accuracy 0.4424\n",
            "Epoch 3 Batch 1350 Loss 1.2857 Accuracy 0.4430\n",
            "Epoch 3 Batch 1400 Loss 1.2824 Accuracy 0.4436\n",
            "Epoch 3 Batch 1450 Loss 1.2798 Accuracy 0.4443\n",
            "Epoch 3 Batch 1500 Loss 1.2774 Accuracy 0.4452\n",
            "Epoch 3 Batch 1550 Loss 1.2748 Accuracy 0.4460\n",
            "Epoch 3 Batch 1600 Loss 1.2717 Accuracy 0.4469\n",
            "Epoch 3 Batch 1650 Loss 1.2694 Accuracy 0.4479\n",
            "Epoch 3 Batch 1700 Loss 1.2666 Accuracy 0.4487\n",
            "Epoch 3 Batch 1750 Loss 1.2641 Accuracy 0.4495\n",
            "Epoch 3 Batch 1800 Loss 1.2622 Accuracy 0.4503\n",
            "Epoch 3 Batch 1850 Loss 1.2593 Accuracy 0.4511\n",
            "Epoch 3 Batch 1900 Loss 1.2559 Accuracy 0.4519\n",
            "Epoch 3 Batch 1950 Loss 1.2534 Accuracy 0.4526\n",
            "Epoch 3 Batch 2000 Loss 1.2505 Accuracy 0.4534\n",
            "Epoch 3 Batch 2050 Loss 1.2482 Accuracy 0.4540\n",
            "Epoch 3 Batch 2100 Loss 1.2459 Accuracy 0.4545\n",
            "Epoch 3 Batch 2150 Loss 1.2429 Accuracy 0.4549\n",
            "Epoch 3 Batch 2200 Loss 1.2396 Accuracy 0.4550\n",
            "Epoch 3 Batch 2250 Loss 1.2359 Accuracy 0.4552\n",
            "Epoch 3 Batch 2300 Loss 1.2325 Accuracy 0.4554\n",
            "Epoch 3 Batch 2350 Loss 1.2291 Accuracy 0.4557\n",
            "Epoch 3 Batch 2400 Loss 1.2253 Accuracy 0.4561\n",
            "Epoch 3 Batch 2450 Loss 1.2220 Accuracy 0.4564\n",
            "Epoch 3 Batch 2500 Loss 1.2188 Accuracy 0.4567\n",
            "Epoch 3 Batch 2550 Loss 1.2158 Accuracy 0.4571\n",
            "Epoch 3 Batch 2600 Loss 1.2129 Accuracy 0.4576\n",
            "Epoch 3 Batch 2650 Loss 1.2098 Accuracy 0.4580\n",
            "Epoch 3 Batch 2700 Loss 1.2066 Accuracy 0.4584\n",
            "Epoch 3 Batch 2750 Loss 1.2039 Accuracy 0.4588\n",
            "Epoch 3 Batch 2800 Loss 1.2008 Accuracy 0.4592\n",
            "Epoch 3 Batch 2850 Loss 1.1977 Accuracy 0.4595\n",
            "Epoch 3 Batch 2900 Loss 1.1954 Accuracy 0.4598\n",
            "Epoch 3 Batch 2950 Loss 1.1927 Accuracy 0.4602\n",
            "Epoch 3 Batch 3000 Loss 1.1901 Accuracy 0.4607\n",
            "Epoch 3 Batch 3050 Loss 1.1874 Accuracy 0.4610\n",
            "Epoch 3 Batch 3100 Loss 1.1851 Accuracy 0.4614\n",
            "Epoch 3 Batch 3150 Loss 1.1827 Accuracy 0.4617\n",
            "Epoch 3 Batch 3200 Loss 1.1805 Accuracy 0.4620\n",
            "Epoch 3 Batch 3250 Loss 1.1781 Accuracy 0.4623\n",
            "Epoch 3 Batch 3300 Loss 1.1755 Accuracy 0.4627\n",
            "Epoch 3 Batch 3350 Loss 1.1729 Accuracy 0.4631\n",
            "Epoch 3 Batch 3400 Loss 1.1705 Accuracy 0.4635\n",
            "Epoch 3 Batch 3450 Loss 1.1682 Accuracy 0.4638\n",
            "Epoch 3 Batch 3500 Loss 1.1656 Accuracy 0.4642\n",
            "Epoch 3 Batch 3550 Loss 1.1633 Accuracy 0.4646\n",
            "Epoch 3 Batch 3600 Loss 1.1610 Accuracy 0.4650\n",
            "Epoch 3 Batch 3650 Loss 1.1588 Accuracy 0.4654\n",
            "Epoch 3 Batch 3700 Loss 1.1566 Accuracy 0.4658\n",
            "Epoch 3 Batch 3750 Loss 1.1545 Accuracy 0.4662\n",
            "Epoch 3 Batch 3800 Loss 1.1525 Accuracy 0.4666\n",
            "Epoch 3 Batch 3850 Loss 1.1506 Accuracy 0.4669\n",
            "Epoch 3 Batch 3900 Loss 1.1488 Accuracy 0.4673\n",
            "Epoch 3 Batch 3950 Loss 1.1471 Accuracy 0.4678\n",
            "Epoch 3 Batch 4000 Loss 1.1454 Accuracy 0.4681\n",
            "Epoch 3 Batch 4050 Loss 1.1436 Accuracy 0.4684\n",
            "Epoch 3 Batch 4100 Loss 1.1422 Accuracy 0.4687\n",
            "Epoch 3 Batch 4150 Loss 1.1414 Accuracy 0.4688\n",
            "Epoch 3 Batch 4200 Loss 1.1411 Accuracy 0.4689\n",
            "Epoch 3 Batch 4250 Loss 1.1413 Accuracy 0.4689\n",
            "Epoch 3 Batch 4300 Loss 1.1416 Accuracy 0.4689\n",
            "Epoch 3 Batch 4350 Loss 1.1421 Accuracy 0.4689\n",
            "Epoch 3 Batch 4400 Loss 1.1427 Accuracy 0.4688\n",
            "Epoch 3 Batch 4450 Loss 1.1435 Accuracy 0.4687\n",
            "Epoch 3 Batch 4500 Loss 1.1442 Accuracy 0.4687\n",
            "Epoch 3 Batch 4550 Loss 1.1450 Accuracy 0.4686\n",
            "Epoch 3 Batch 4600 Loss 1.1463 Accuracy 0.4684\n",
            "Epoch 3 Batch 4650 Loss 1.1473 Accuracy 0.4683\n",
            "Epoch 3 Batch 4700 Loss 1.1483 Accuracy 0.4682\n",
            "Epoch 3 Batch 4750 Loss 1.1493 Accuracy 0.4680\n",
            "Epoch 3 Batch 4800 Loss 1.1505 Accuracy 0.4679\n",
            "Epoch 3 Batch 4850 Loss 1.1513 Accuracy 0.4678\n",
            "Epoch 3 Batch 4900 Loss 1.1522 Accuracy 0.4677\n",
            "Epoch 3 Batch 4950 Loss 1.1529 Accuracy 0.4675\n",
            "Epoch 3 Batch 5000 Loss 1.1537 Accuracy 0.4674\n",
            "Epoch 3 Batch 5050 Loss 1.1548 Accuracy 0.4673\n",
            "Epoch 3 Batch 5100 Loss 1.1553 Accuracy 0.4671\n",
            "Epoch 3 Batch 5150 Loss 1.1563 Accuracy 0.4670\n",
            "Epoch 3 Batch 5200 Loss 1.1573 Accuracy 0.4667\n",
            "Epoch 3 Batch 5250 Loss 1.1580 Accuracy 0.4666\n",
            "Epoch 3 Batch 5300 Loss 1.1588 Accuracy 0.4663\n",
            "Epoch 3 Batch 5350 Loss 1.1595 Accuracy 0.4660\n",
            "Epoch 3 Batch 5400 Loss 1.1600 Accuracy 0.4658\n",
            "Epoch 3 Batch 5450 Loss 1.1607 Accuracy 0.4656\n",
            "Epoch 3 Batch 5500 Loss 1.1611 Accuracy 0.4655\n",
            "Epoch 3 Batch 5550 Loss 1.1616 Accuracy 0.4653\n",
            "Epoch 3 Batch 5600 Loss 1.1622 Accuracy 0.4651\n",
            "Epoch 3 Batch 5650 Loss 1.1626 Accuracy 0.4650\n",
            "Epoch 3 Batch 5700 Loss 1.1631 Accuracy 0.4648\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/projects/transformer/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 2075.6673719882965 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.0095 Accuracy 0.4745\n",
            "Epoch 4 Batch 50 Loss 1.1967 Accuracy 0.4538\n",
            "Epoch 4 Batch 100 Loss 1.1975 Accuracy 0.4576\n",
            "Epoch 4 Batch 150 Loss 1.1959 Accuracy 0.4576\n",
            "Epoch 4 Batch 200 Loss 1.1998 Accuracy 0.4582\n",
            "Epoch 4 Batch 250 Loss 1.2031 Accuracy 0.4577\n",
            "Epoch 4 Batch 300 Loss 1.2087 Accuracy 0.4578\n",
            "Epoch 4 Batch 350 Loss 1.2063 Accuracy 0.4567\n",
            "Epoch 4 Batch 400 Loss 1.2061 Accuracy 0.4571\n",
            "Epoch 4 Batch 450 Loss 1.2030 Accuracy 0.4570\n",
            "Epoch 4 Batch 500 Loss 1.2009 Accuracy 0.4566\n",
            "Epoch 4 Batch 550 Loss 1.2006 Accuracy 0.4567\n",
            "Epoch 4 Batch 600 Loss 1.2003 Accuracy 0.4563\n",
            "Epoch 4 Batch 650 Loss 1.1987 Accuracy 0.4566\n",
            "Epoch 4 Batch 700 Loss 1.1968 Accuracy 0.4569\n",
            "Epoch 4 Batch 750 Loss 1.1961 Accuracy 0.4570\n",
            "Epoch 4 Batch 800 Loss 1.1964 Accuracy 0.4572\n",
            "Epoch 4 Batch 850 Loss 1.1960 Accuracy 0.4573\n",
            "Epoch 4 Batch 900 Loss 1.1958 Accuracy 0.4570\n",
            "Epoch 4 Batch 950 Loss 1.1939 Accuracy 0.4570\n",
            "Epoch 4 Batch 1000 Loss 1.1909 Accuracy 0.4571\n",
            "Epoch 4 Batch 1050 Loss 1.1895 Accuracy 0.4573\n",
            "Epoch 4 Batch 1100 Loss 1.1878 Accuracy 0.4575\n",
            "Epoch 4 Batch 1150 Loss 1.1869 Accuracy 0.4577\n",
            "Epoch 4 Batch 1200 Loss 1.1863 Accuracy 0.4579\n",
            "Epoch 4 Batch 1250 Loss 1.1841 Accuracy 0.4582\n",
            "Epoch 4 Batch 1300 Loss 1.1815 Accuracy 0.4586\n",
            "Epoch 4 Batch 1350 Loss 1.1793 Accuracy 0.4594\n",
            "Epoch 4 Batch 1400 Loss 1.1768 Accuracy 0.4600\n",
            "Epoch 4 Batch 1450 Loss 1.1734 Accuracy 0.4607\n",
            "Epoch 4 Batch 1500 Loss 1.1709 Accuracy 0.4615\n",
            "Epoch 4 Batch 1550 Loss 1.1680 Accuracy 0.4625\n",
            "Epoch 4 Batch 1600 Loss 1.1653 Accuracy 0.4635\n",
            "Epoch 4 Batch 1650 Loss 1.1626 Accuracy 0.4644\n",
            "Epoch 4 Batch 1700 Loss 1.1606 Accuracy 0.4652\n",
            "Epoch 4 Batch 1750 Loss 1.1580 Accuracy 0.4659\n",
            "Epoch 4 Batch 1800 Loss 1.1557 Accuracy 0.4667\n",
            "Epoch 4 Batch 1850 Loss 1.1534 Accuracy 0.4675\n",
            "Epoch 4 Batch 1900 Loss 1.1509 Accuracy 0.4683\n",
            "Epoch 4 Batch 1950 Loss 1.1495 Accuracy 0.4689\n",
            "Epoch 4 Batch 2000 Loss 1.1469 Accuracy 0.4697\n",
            "Epoch 4 Batch 2050 Loss 1.1443 Accuracy 0.4701\n",
            "Epoch 4 Batch 2100 Loss 1.1413 Accuracy 0.4706\n",
            "Epoch 4 Batch 2150 Loss 1.1382 Accuracy 0.4708\n",
            "Epoch 4 Batch 2200 Loss 1.1350 Accuracy 0.4711\n",
            "Epoch 4 Batch 2250 Loss 1.1319 Accuracy 0.4713\n",
            "Epoch 4 Batch 2300 Loss 1.1290 Accuracy 0.4715\n",
            "Epoch 4 Batch 2350 Loss 1.1260 Accuracy 0.4717\n",
            "Epoch 4 Batch 2400 Loss 1.1228 Accuracy 0.4720\n",
            "Epoch 4 Batch 2450 Loss 1.1200 Accuracy 0.4723\n",
            "Epoch 4 Batch 2500 Loss 1.1169 Accuracy 0.4725\n",
            "Epoch 4 Batch 2550 Loss 1.1136 Accuracy 0.4729\n",
            "Epoch 4 Batch 2600 Loss 1.1109 Accuracy 0.4732\n",
            "Epoch 4 Batch 2650 Loss 1.1080 Accuracy 0.4736\n",
            "Epoch 4 Batch 2700 Loss 1.1055 Accuracy 0.4740\n",
            "Epoch 4 Batch 2750 Loss 1.1032 Accuracy 0.4743\n",
            "Epoch 4 Batch 2800 Loss 1.1008 Accuracy 0.4748\n",
            "Epoch 4 Batch 2850 Loss 1.0986 Accuracy 0.4752\n",
            "Epoch 4 Batch 2900 Loss 1.0965 Accuracy 0.4755\n",
            "Epoch 4 Batch 2950 Loss 1.0946 Accuracy 0.4759\n",
            "Epoch 4 Batch 3000 Loss 1.0922 Accuracy 0.4762\n",
            "Epoch 4 Batch 3050 Loss 1.0901 Accuracy 0.4763\n",
            "Epoch 4 Batch 3100 Loss 1.0883 Accuracy 0.4766\n",
            "Epoch 4 Batch 3150 Loss 1.0862 Accuracy 0.4769\n",
            "Epoch 4 Batch 3200 Loss 1.0838 Accuracy 0.4772\n",
            "Epoch 4 Batch 3250 Loss 1.0814 Accuracy 0.4775\n",
            "Epoch 4 Batch 3300 Loss 1.0789 Accuracy 0.4778\n",
            "Epoch 4 Batch 3350 Loss 1.0771 Accuracy 0.4781\n",
            "Epoch 4 Batch 3400 Loss 1.0750 Accuracy 0.4785\n",
            "Epoch 4 Batch 3450 Loss 1.0726 Accuracy 0.4788\n",
            "Epoch 4 Batch 3500 Loss 1.0706 Accuracy 0.4791\n",
            "Epoch 4 Batch 3550 Loss 1.0685 Accuracy 0.4795\n",
            "Epoch 4 Batch 3600 Loss 1.0667 Accuracy 0.4798\n",
            "Epoch 4 Batch 3650 Loss 1.0646 Accuracy 0.4802\n",
            "Epoch 4 Batch 3700 Loss 1.0628 Accuracy 0.4806\n",
            "Epoch 4 Batch 3750 Loss 1.0609 Accuracy 0.4810\n",
            "Epoch 4 Batch 3800 Loss 1.0590 Accuracy 0.4814\n",
            "Epoch 4 Batch 3850 Loss 1.0573 Accuracy 0.4817\n",
            "Epoch 4 Batch 3900 Loss 1.0555 Accuracy 0.4821\n",
            "Epoch 4 Batch 3950 Loss 1.0541 Accuracy 0.4824\n",
            "Epoch 4 Batch 4000 Loss 1.0527 Accuracy 0.4827\n",
            "Epoch 4 Batch 4050 Loss 1.0513 Accuracy 0.4831\n",
            "Epoch 4 Batch 4100 Loss 1.0502 Accuracy 0.4834\n",
            "Epoch 4 Batch 4150 Loss 1.0494 Accuracy 0.4835\n",
            "Epoch 4 Batch 4200 Loss 1.0494 Accuracy 0.4835\n",
            "Epoch 4 Batch 4250 Loss 1.0499 Accuracy 0.4835\n",
            "Epoch 4 Batch 4300 Loss 1.0504 Accuracy 0.4835\n",
            "Epoch 4 Batch 4350 Loss 1.0510 Accuracy 0.4834\n",
            "Epoch 4 Batch 4400 Loss 1.0519 Accuracy 0.4832\n",
            "Epoch 4 Batch 4450 Loss 1.0530 Accuracy 0.4830\n",
            "Epoch 4 Batch 4500 Loss 1.0539 Accuracy 0.4828\n",
            "Epoch 4 Batch 4550 Loss 1.0553 Accuracy 0.4826\n",
            "Epoch 4 Batch 4600 Loss 1.0561 Accuracy 0.4825\n",
            "Epoch 4 Batch 4650 Loss 1.0573 Accuracy 0.4823\n",
            "Epoch 4 Batch 4700 Loss 1.0584 Accuracy 0.4822\n",
            "Epoch 4 Batch 4750 Loss 1.0593 Accuracy 0.4820\n",
            "Epoch 4 Batch 4800 Loss 1.0604 Accuracy 0.4819\n",
            "Epoch 4 Batch 4850 Loss 1.0613 Accuracy 0.4817\n",
            "Epoch 4 Batch 4900 Loss 1.0624 Accuracy 0.4816\n",
            "Epoch 4 Batch 4950 Loss 1.0637 Accuracy 0.4815\n",
            "Epoch 4 Batch 5000 Loss 1.0647 Accuracy 0.4813\n",
            "Epoch 4 Batch 5050 Loss 1.0659 Accuracy 0.4811\n",
            "Epoch 4 Batch 5100 Loss 1.0671 Accuracy 0.4809\n",
            "Epoch 4 Batch 5150 Loss 1.0681 Accuracy 0.4807\n",
            "Epoch 4 Batch 5200 Loss 1.0694 Accuracy 0.4805\n",
            "Epoch 4 Batch 5250 Loss 1.0703 Accuracy 0.4802\n",
            "Epoch 4 Batch 5300 Loss 1.0712 Accuracy 0.4800\n",
            "Epoch 4 Batch 5350 Loss 1.0723 Accuracy 0.4797\n",
            "Epoch 4 Batch 5400 Loss 1.0731 Accuracy 0.4795\n",
            "Epoch 4 Batch 5450 Loss 1.0741 Accuracy 0.4793\n",
            "Epoch 4 Batch 5500 Loss 1.0750 Accuracy 0.4791\n",
            "Epoch 4 Batch 5550 Loss 1.0757 Accuracy 0.4788\n",
            "Epoch 4 Batch 5600 Loss 1.0764 Accuracy 0.4786\n",
            "Epoch 4 Batch 5650 Loss 1.0772 Accuracy 0.4784\n",
            "Epoch 4 Batch 5700 Loss 1.0780 Accuracy 0.4782\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/projects/transformer/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 2104.002813577652 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.1404 Accuracy 0.4663\n",
            "Epoch 5 Batch 50 Loss 1.1652 Accuracy 0.4567\n",
            "Epoch 5 Batch 100 Loss 1.1494 Accuracy 0.4621\n",
            "Epoch 5 Batch 150 Loss 1.1475 Accuracy 0.4641\n",
            "Epoch 5 Batch 200 Loss 1.1533 Accuracy 0.4643\n",
            "Epoch 5 Batch 250 Loss 1.1558 Accuracy 0.4650\n",
            "Epoch 5 Batch 300 Loss 1.1523 Accuracy 0.4656\n",
            "Epoch 5 Batch 350 Loss 1.1461 Accuracy 0.4656\n",
            "Epoch 5 Batch 400 Loss 1.1425 Accuracy 0.4666\n",
            "Epoch 5 Batch 450 Loss 1.1424 Accuracy 0.4664\n",
            "Epoch 5 Batch 500 Loss 1.1405 Accuracy 0.4660\n",
            "Epoch 5 Batch 550 Loss 1.1399 Accuracy 0.4661\n",
            "Epoch 5 Batch 600 Loss 1.1389 Accuracy 0.4663\n",
            "Epoch 5 Batch 650 Loss 1.1385 Accuracy 0.4665\n",
            "Epoch 5 Batch 700 Loss 1.1377 Accuracy 0.4666\n",
            "Epoch 5 Batch 750 Loss 1.1366 Accuracy 0.4670\n",
            "Epoch 5 Batch 800 Loss 1.1361 Accuracy 0.4672\n",
            "Epoch 5 Batch 850 Loss 1.1342 Accuracy 0.4674\n",
            "Epoch 5 Batch 900 Loss 1.1345 Accuracy 0.4676\n",
            "Epoch 5 Batch 950 Loss 1.1327 Accuracy 0.4675\n",
            "Epoch 5 Batch 1000 Loss 1.1314 Accuracy 0.4676\n",
            "Epoch 5 Batch 1050 Loss 1.1300 Accuracy 0.4679\n",
            "Epoch 5 Batch 1100 Loss 1.1287 Accuracy 0.4679\n",
            "Epoch 5 Batch 1150 Loss 1.1268 Accuracy 0.4682\n",
            "Epoch 5 Batch 1200 Loss 1.1251 Accuracy 0.4683\n",
            "Epoch 5 Batch 1250 Loss 1.1237 Accuracy 0.4686\n",
            "Epoch 5 Batch 1300 Loss 1.1210 Accuracy 0.4690\n",
            "Epoch 5 Batch 1350 Loss 1.1188 Accuracy 0.4697\n",
            "Epoch 5 Batch 1400 Loss 1.1162 Accuracy 0.4703\n",
            "Epoch 5 Batch 1450 Loss 1.1131 Accuracy 0.4712\n",
            "Epoch 5 Batch 1500 Loss 1.1098 Accuracy 0.4721\n",
            "Epoch 5 Batch 1550 Loss 1.1072 Accuracy 0.4729\n",
            "Epoch 5 Batch 1600 Loss 1.1051 Accuracy 0.4737\n",
            "Epoch 5 Batch 1650 Loss 1.1035 Accuracy 0.4744\n",
            "Epoch 5 Batch 1700 Loss 1.1010 Accuracy 0.4752\n",
            "Epoch 5 Batch 1750 Loss 1.0988 Accuracy 0.4758\n",
            "Epoch 5 Batch 1800 Loss 1.0966 Accuracy 0.4767\n",
            "Epoch 5 Batch 1850 Loss 1.0940 Accuracy 0.4774\n",
            "Epoch 5 Batch 1900 Loss 1.0912 Accuracy 0.4781\n",
            "Epoch 5 Batch 1950 Loss 1.0893 Accuracy 0.4790\n",
            "Epoch 5 Batch 2000 Loss 1.0872 Accuracy 0.4796\n",
            "Epoch 5 Batch 2050 Loss 1.0847 Accuracy 0.4801\n",
            "Epoch 5 Batch 2100 Loss 1.0821 Accuracy 0.4805\n",
            "Epoch 5 Batch 2150 Loss 1.0790 Accuracy 0.4807\n",
            "Epoch 5 Batch 2200 Loss 1.0761 Accuracy 0.4809\n",
            "Epoch 5 Batch 2250 Loss 1.0728 Accuracy 0.4810\n",
            "Epoch 5 Batch 2300 Loss 1.0696 Accuracy 0.4813\n",
            "Epoch 5 Batch 2350 Loss 1.0666 Accuracy 0.4815\n",
            "Epoch 5 Batch 2400 Loss 1.0642 Accuracy 0.4816\n",
            "Epoch 5 Batch 2450 Loss 1.0612 Accuracy 0.4820\n",
            "Epoch 5 Batch 2500 Loss 1.0587 Accuracy 0.4824\n",
            "Epoch 5 Batch 2550 Loss 1.0561 Accuracy 0.4827\n",
            "Epoch 5 Batch 2600 Loss 1.0536 Accuracy 0.4831\n",
            "Epoch 5 Batch 2650 Loss 1.0510 Accuracy 0.4834\n",
            "Epoch 5 Batch 2700 Loss 1.0484 Accuracy 0.4837\n",
            "Epoch 5 Batch 2750 Loss 1.0457 Accuracy 0.4841\n",
            "Epoch 5 Batch 2800 Loss 1.0433 Accuracy 0.4844\n",
            "Epoch 5 Batch 2850 Loss 1.0403 Accuracy 0.4847\n",
            "Epoch 5 Batch 2900 Loss 1.0379 Accuracy 0.4851\n",
            "Epoch 5 Batch 2950 Loss 1.0360 Accuracy 0.4855\n",
            "Epoch 5 Batch 3000 Loss 1.0338 Accuracy 0.4858\n",
            "Epoch 5 Batch 3050 Loss 1.0321 Accuracy 0.4861\n",
            "Epoch 5 Batch 3100 Loss 1.0300 Accuracy 0.4864\n",
            "Epoch 5 Batch 3150 Loss 1.0277 Accuracy 0.4867\n",
            "Epoch 5 Batch 3200 Loss 1.0258 Accuracy 0.4868\n",
            "Epoch 5 Batch 3250 Loss 1.0237 Accuracy 0.4871\n",
            "Epoch 5 Batch 3300 Loss 1.0217 Accuracy 0.4873\n",
            "Epoch 5 Batch 3350 Loss 1.0198 Accuracy 0.4876\n",
            "Epoch 5 Batch 3400 Loss 1.0177 Accuracy 0.4880\n",
            "Epoch 5 Batch 3450 Loss 1.0161 Accuracy 0.4882\n",
            "Epoch 5 Batch 3500 Loss 1.0143 Accuracy 0.4885\n",
            "Epoch 5 Batch 3550 Loss 1.0124 Accuracy 0.4888\n",
            "Epoch 5 Batch 3600 Loss 1.0108 Accuracy 0.4891\n",
            "Epoch 5 Batch 3650 Loss 1.0091 Accuracy 0.4894\n",
            "Epoch 5 Batch 3700 Loss 1.0074 Accuracy 0.4897\n",
            "Epoch 5 Batch 3750 Loss 1.0055 Accuracy 0.4901\n",
            "Epoch 5 Batch 3800 Loss 1.0039 Accuracy 0.4904\n",
            "Epoch 5 Batch 3850 Loss 1.0023 Accuracy 0.4907\n",
            "Epoch 5 Batch 3900 Loss 1.0011 Accuracy 0.4910\n",
            "Epoch 5 Batch 3950 Loss 0.9998 Accuracy 0.4913\n",
            "Epoch 5 Batch 4000 Loss 0.9981 Accuracy 0.4917\n",
            "Epoch 5 Batch 4050 Loss 0.9968 Accuracy 0.4921\n",
            "Epoch 5 Batch 4100 Loss 0.9956 Accuracy 0.4922\n",
            "Epoch 5 Batch 4150 Loss 0.9950 Accuracy 0.4923\n",
            "Epoch 5 Batch 4200 Loss 0.9951 Accuracy 0.4923\n",
            "Epoch 5 Batch 4250 Loss 0.9954 Accuracy 0.4923\n",
            "Epoch 5 Batch 4300 Loss 0.9961 Accuracy 0.4922\n",
            "Epoch 5 Batch 4350 Loss 0.9968 Accuracy 0.4922\n",
            "Epoch 5 Batch 4400 Loss 0.9979 Accuracy 0.4920\n",
            "Epoch 5 Batch 4450 Loss 0.9989 Accuracy 0.4918\n",
            "Epoch 5 Batch 4500 Loss 1.0002 Accuracy 0.4917\n",
            "Epoch 5 Batch 4550 Loss 1.0017 Accuracy 0.4915\n",
            "Epoch 5 Batch 4600 Loss 1.0029 Accuracy 0.4913\n",
            "Epoch 5 Batch 4650 Loss 1.0041 Accuracy 0.4912\n",
            "Epoch 5 Batch 4700 Loss 1.0055 Accuracy 0.4910\n",
            "Epoch 5 Batch 4750 Loss 1.0066 Accuracy 0.4908\n",
            "Epoch 5 Batch 4800 Loss 1.0077 Accuracy 0.4907\n",
            "Epoch 5 Batch 4850 Loss 1.0088 Accuracy 0.4905\n",
            "Epoch 5 Batch 4900 Loss 1.0097 Accuracy 0.4904\n",
            "Epoch 5 Batch 4950 Loss 1.0109 Accuracy 0.4902\n",
            "Epoch 5 Batch 5000 Loss 1.0122 Accuracy 0.4900\n",
            "Epoch 5 Batch 5050 Loss 1.0134 Accuracy 0.4898\n",
            "Epoch 5 Batch 5100 Loss 1.0147 Accuracy 0.4895\n",
            "Epoch 5 Batch 5150 Loss 1.0158 Accuracy 0.4893\n",
            "Epoch 5 Batch 5200 Loss 1.0172 Accuracy 0.4891\n",
            "Epoch 5 Batch 5250 Loss 1.0180 Accuracy 0.4888\n",
            "Epoch 5 Batch 5300 Loss 1.0190 Accuracy 0.4885\n",
            "Epoch 5 Batch 5350 Loss 1.0200 Accuracy 0.4883\n",
            "Epoch 5 Batch 5400 Loss 1.0209 Accuracy 0.4880\n",
            "Epoch 5 Batch 5450 Loss 1.0220 Accuracy 0.4877\n",
            "Epoch 5 Batch 5500 Loss 1.0231 Accuracy 0.4875\n",
            "Epoch 5 Batch 5550 Loss 1.0241 Accuracy 0.4873\n",
            "Epoch 5 Batch 5600 Loss 1.0247 Accuracy 0.4871\n",
            "Epoch 5 Batch 5650 Loss 1.0254 Accuracy 0.4869\n",
            "Epoch 5 Batch 5700 Loss 1.0262 Accuracy 0.4867\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/projects/transformer/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 2103.4974496364594 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.1546 Accuracy 0.4959\n",
            "Epoch 6 Batch 50 Loss 1.1413 Accuracy 0.4718\n",
            "Epoch 6 Batch 100 Loss 1.1141 Accuracy 0.4726\n",
            "Epoch 6 Batch 150 Loss 1.1130 Accuracy 0.4734\n",
            "Epoch 6 Batch 200 Loss 1.1070 Accuracy 0.4722\n",
            "Epoch 6 Batch 250 Loss 1.1087 Accuracy 0.4712\n",
            "Epoch 6 Batch 300 Loss 1.1088 Accuracy 0.4719\n",
            "Epoch 6 Batch 350 Loss 1.1068 Accuracy 0.4717\n",
            "Epoch 6 Batch 400 Loss 1.1057 Accuracy 0.4713\n",
            "Epoch 6 Batch 450 Loss 1.1018 Accuracy 0.4711\n",
            "Epoch 6 Batch 500 Loss 1.1014 Accuracy 0.4713\n",
            "Epoch 6 Batch 550 Loss 1.1001 Accuracy 0.4714\n",
            "Epoch 6 Batch 600 Loss 1.1000 Accuracy 0.4712\n",
            "Epoch 6 Batch 650 Loss 1.0990 Accuracy 0.4718\n",
            "Epoch 6 Batch 700 Loss 1.0973 Accuracy 0.4727\n",
            "Epoch 6 Batch 750 Loss 1.0960 Accuracy 0.4727\n",
            "Epoch 6 Batch 800 Loss 1.0958 Accuracy 0.4731\n",
            "Epoch 6 Batch 850 Loss 1.0947 Accuracy 0.4738\n",
            "Epoch 6 Batch 900 Loss 1.0928 Accuracy 0.4738\n",
            "Epoch 6 Batch 950 Loss 1.0912 Accuracy 0.4738\n",
            "Epoch 6 Batch 1000 Loss 1.0898 Accuracy 0.4740\n",
            "Epoch 6 Batch 1050 Loss 1.0892 Accuracy 0.4743\n",
            "Epoch 6 Batch 1100 Loss 1.0875 Accuracy 0.4741\n",
            "Epoch 6 Batch 1150 Loss 1.0869 Accuracy 0.4741\n",
            "Epoch 6 Batch 1200 Loss 1.0841 Accuracy 0.4744\n",
            "Epoch 6 Batch 1250 Loss 1.0825 Accuracy 0.4748\n",
            "Epoch 6 Batch 1300 Loss 1.0815 Accuracy 0.4754\n",
            "Epoch 6 Batch 1350 Loss 1.0792 Accuracy 0.4759\n",
            "Epoch 6 Batch 1400 Loss 1.0764 Accuracy 0.4767\n",
            "Epoch 6 Batch 1450 Loss 1.0742 Accuracy 0.4773\n",
            "Epoch 6 Batch 1500 Loss 1.0718 Accuracy 0.4781\n",
            "Epoch 6 Batch 1550 Loss 1.0684 Accuracy 0.4791\n",
            "Epoch 6 Batch 1600 Loss 1.0654 Accuracy 0.4800\n",
            "Epoch 6 Batch 1650 Loss 1.0631 Accuracy 0.4810\n",
            "Epoch 6 Batch 1700 Loss 1.0608 Accuracy 0.4818\n",
            "Epoch 6 Batch 1750 Loss 1.0587 Accuracy 0.4826\n",
            "Epoch 6 Batch 1800 Loss 1.0559 Accuracy 0.4833\n",
            "Epoch 6 Batch 1850 Loss 1.0548 Accuracy 0.4841\n",
            "Epoch 6 Batch 1900 Loss 1.0519 Accuracy 0.4849\n",
            "Epoch 6 Batch 1950 Loss 1.0502 Accuracy 0.4856\n",
            "Epoch 6 Batch 2000 Loss 1.0477 Accuracy 0.4862\n",
            "Epoch 6 Batch 2050 Loss 1.0454 Accuracy 0.4867\n",
            "Epoch 6 Batch 2100 Loss 1.0431 Accuracy 0.4870\n",
            "Epoch 6 Batch 2150 Loss 1.0401 Accuracy 0.4873\n",
            "Epoch 6 Batch 2200 Loss 1.0373 Accuracy 0.4874\n",
            "Epoch 6 Batch 2250 Loss 1.0341 Accuracy 0.4875\n",
            "Epoch 6 Batch 2300 Loss 1.0314 Accuracy 0.4876\n",
            "Epoch 6 Batch 2350 Loss 1.0287 Accuracy 0.4878\n",
            "Epoch 6 Batch 2400 Loss 1.0258 Accuracy 0.4881\n",
            "Epoch 6 Batch 2450 Loss 1.0229 Accuracy 0.4883\n",
            "Epoch 6 Batch 2500 Loss 1.0203 Accuracy 0.4886\n",
            "Epoch 6 Batch 2550 Loss 1.0175 Accuracy 0.4889\n",
            "Epoch 6 Batch 2600 Loss 1.0146 Accuracy 0.4893\n",
            "Epoch 6 Batch 2650 Loss 1.0118 Accuracy 0.4897\n",
            "Epoch 6 Batch 2700 Loss 1.0094 Accuracy 0.4901\n",
            "Epoch 6 Batch 2750 Loss 1.0069 Accuracy 0.4904\n",
            "Epoch 6 Batch 2800 Loss 1.0044 Accuracy 0.4907\n",
            "Epoch 6 Batch 2850 Loss 1.0025 Accuracy 0.4910\n",
            "Epoch 6 Batch 2900 Loss 1.0004 Accuracy 0.4913\n",
            "Epoch 6 Batch 2950 Loss 0.9983 Accuracy 0.4917\n",
            "Epoch 6 Batch 3000 Loss 0.9959 Accuracy 0.4919\n",
            "Epoch 6 Batch 3050 Loss 0.9940 Accuracy 0.4922\n",
            "Epoch 6 Batch 3100 Loss 0.9922 Accuracy 0.4924\n",
            "Epoch 6 Batch 3150 Loss 0.9904 Accuracy 0.4928\n",
            "Epoch 6 Batch 3200 Loss 0.9882 Accuracy 0.4930\n",
            "Epoch 6 Batch 3250 Loss 0.9860 Accuracy 0.4932\n",
            "Epoch 6 Batch 3300 Loss 0.9841 Accuracy 0.4935\n",
            "Epoch 6 Batch 3350 Loss 0.9822 Accuracy 0.4938\n",
            "Epoch 6 Batch 3400 Loss 0.9803 Accuracy 0.4940\n",
            "Epoch 6 Batch 3450 Loss 0.9783 Accuracy 0.4943\n",
            "Epoch 6 Batch 3500 Loss 0.9768 Accuracy 0.4946\n",
            "Epoch 6 Batch 3550 Loss 0.9750 Accuracy 0.4949\n",
            "Epoch 6 Batch 3600 Loss 0.9732 Accuracy 0.4951\n",
            "Epoch 6 Batch 3650 Loss 0.9715 Accuracy 0.4955\n",
            "Epoch 6 Batch 3700 Loss 0.9697 Accuracy 0.4959\n",
            "Epoch 6 Batch 3750 Loss 0.9681 Accuracy 0.4963\n",
            "Epoch 6 Batch 3800 Loss 0.9667 Accuracy 0.4966\n",
            "Epoch 6 Batch 3850 Loss 0.9652 Accuracy 0.4970\n",
            "Epoch 6 Batch 3900 Loss 0.9639 Accuracy 0.4973\n",
            "Epoch 6 Batch 3950 Loss 0.9624 Accuracy 0.4977\n",
            "Epoch 6 Batch 4000 Loss 0.9611 Accuracy 0.4980\n",
            "Epoch 6 Batch 4050 Loss 0.9597 Accuracy 0.4982\n",
            "Epoch 6 Batch 4100 Loss 0.9585 Accuracy 0.4985\n",
            "Epoch 6 Batch 4150 Loss 0.9583 Accuracy 0.4986\n",
            "Epoch 6 Batch 4200 Loss 0.9584 Accuracy 0.4986\n",
            "Epoch 6 Batch 4250 Loss 0.9588 Accuracy 0.4985\n",
            "Epoch 6 Batch 4300 Loss 0.9595 Accuracy 0.4984\n",
            "Epoch 6 Batch 4350 Loss 0.9602 Accuracy 0.4983\n",
            "Epoch 6 Batch 4400 Loss 0.9614 Accuracy 0.4981\n",
            "Epoch 6 Batch 4450 Loss 0.9625 Accuracy 0.4980\n",
            "Epoch 6 Batch 4500 Loss 0.9637 Accuracy 0.4979\n",
            "Epoch 6 Batch 4550 Loss 0.9650 Accuracy 0.4977\n",
            "Epoch 6 Batch 4600 Loss 0.9666 Accuracy 0.4975\n",
            "Epoch 6 Batch 4650 Loss 0.9681 Accuracy 0.4973\n",
            "Epoch 6 Batch 4700 Loss 0.9696 Accuracy 0.4972\n",
            "Epoch 6 Batch 4750 Loss 0.9706 Accuracy 0.4970\n",
            "Epoch 6 Batch 4800 Loss 0.9718 Accuracy 0.4968\n",
            "Epoch 6 Batch 4850 Loss 0.9730 Accuracy 0.4966\n",
            "Epoch 6 Batch 4900 Loss 0.9743 Accuracy 0.4964\n",
            "Epoch 6 Batch 4950 Loss 0.9755 Accuracy 0.4963\n",
            "Epoch 6 Batch 5000 Loss 0.9768 Accuracy 0.4961\n",
            "Epoch 6 Batch 5050 Loss 0.9779 Accuracy 0.4958\n",
            "Epoch 6 Batch 5100 Loss 0.9791 Accuracy 0.4956\n",
            "Epoch 6 Batch 5150 Loss 0.9804 Accuracy 0.4954\n",
            "Epoch 6 Batch 5200 Loss 0.9817 Accuracy 0.4951\n",
            "Epoch 6 Batch 5250 Loss 0.9829 Accuracy 0.4949\n",
            "Epoch 6 Batch 5300 Loss 0.9840 Accuracy 0.4945\n",
            "Epoch 6 Batch 5350 Loss 0.9850 Accuracy 0.4943\n",
            "Epoch 6 Batch 5400 Loss 0.9860 Accuracy 0.4940\n",
            "Epoch 6 Batch 5450 Loss 0.9871 Accuracy 0.4938\n",
            "Epoch 6 Batch 5500 Loss 0.9878 Accuracy 0.4935\n",
            "Epoch 6 Batch 5550 Loss 0.9887 Accuracy 0.4933\n",
            "Epoch 6 Batch 5600 Loss 0.9894 Accuracy 0.4931\n",
            "Epoch 6 Batch 5650 Loss 0.9903 Accuracy 0.4928\n",
            "Epoch 6 Batch 5700 Loss 0.9911 Accuracy 0.4926\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/projects/transformer/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 2113.394302368164 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.1049 Accuracy 0.4441\n",
            "Epoch 7 Batch 50 Loss 1.1063 Accuracy 0.4722\n",
            "Epoch 7 Batch 100 Loss 1.0921 Accuracy 0.4750\n",
            "Epoch 7 Batch 150 Loss 1.0868 Accuracy 0.4744\n",
            "Epoch 7 Batch 200 Loss 1.0806 Accuracy 0.4756\n",
            "Epoch 7 Batch 250 Loss 1.0852 Accuracy 0.4757\n",
            "Epoch 7 Batch 300 Loss 1.0821 Accuracy 0.4755\n",
            "Epoch 7 Batch 350 Loss 1.0823 Accuracy 0.4756\n",
            "Epoch 7 Batch 400 Loss 1.0805 Accuracy 0.4763\n",
            "Epoch 7 Batch 450 Loss 1.0760 Accuracy 0.4760\n",
            "Epoch 7 Batch 500 Loss 1.0763 Accuracy 0.4765\n",
            "Epoch 7 Batch 550 Loss 1.0739 Accuracy 0.4764\n",
            "Epoch 7 Batch 600 Loss 1.0717 Accuracy 0.4765\n",
            "Epoch 7 Batch 650 Loss 1.0703 Accuracy 0.4773\n",
            "Epoch 7 Batch 700 Loss 1.0684 Accuracy 0.4777\n",
            "Epoch 7 Batch 750 Loss 1.0678 Accuracy 0.4782\n",
            "Epoch 7 Batch 800 Loss 1.0673 Accuracy 0.4788\n",
            "Epoch 7 Batch 850 Loss 1.0666 Accuracy 0.4789\n",
            "Epoch 7 Batch 900 Loss 1.0660 Accuracy 0.4789\n",
            "Epoch 7 Batch 950 Loss 1.0650 Accuracy 0.4789\n",
            "Epoch 7 Batch 1000 Loss 1.0634 Accuracy 0.4790\n",
            "Epoch 7 Batch 1050 Loss 1.0611 Accuracy 0.4791\n",
            "Epoch 7 Batch 1100 Loss 1.0588 Accuracy 0.4793\n",
            "Epoch 7 Batch 1150 Loss 1.0575 Accuracy 0.4795\n",
            "Epoch 7 Batch 1200 Loss 1.0552 Accuracy 0.4797\n",
            "Epoch 7 Batch 1250 Loss 1.0534 Accuracy 0.4801\n",
            "Epoch 7 Batch 1300 Loss 1.0519 Accuracy 0.4805\n",
            "Epoch 7 Batch 1350 Loss 1.0487 Accuracy 0.4810\n",
            "Epoch 7 Batch 1400 Loss 1.0463 Accuracy 0.4815\n",
            "Epoch 7 Batch 1450 Loss 1.0441 Accuracy 0.4822\n",
            "Epoch 7 Batch 1500 Loss 1.0416 Accuracy 0.4829\n",
            "Epoch 7 Batch 1550 Loss 1.0392 Accuracy 0.4838\n",
            "Epoch 7 Batch 1600 Loss 1.0366 Accuracy 0.4847\n",
            "Epoch 7 Batch 1650 Loss 1.0345 Accuracy 0.4856\n",
            "Epoch 7 Batch 1700 Loss 1.0315 Accuracy 0.4864\n",
            "Epoch 7 Batch 1750 Loss 1.0292 Accuracy 0.4872\n",
            "Epoch 7 Batch 1800 Loss 1.0273 Accuracy 0.4880\n",
            "Epoch 7 Batch 1850 Loss 1.0248 Accuracy 0.4887\n",
            "Epoch 7 Batch 1900 Loss 1.0225 Accuracy 0.4892\n",
            "Epoch 7 Batch 1950 Loss 1.0202 Accuracy 0.4901\n",
            "Epoch 7 Batch 2000 Loss 1.0186 Accuracy 0.4907\n",
            "Epoch 7 Batch 2050 Loss 1.0163 Accuracy 0.4912\n",
            "Epoch 7 Batch 2100 Loss 1.0139 Accuracy 0.4916\n",
            "Epoch 7 Batch 2150 Loss 1.0110 Accuracy 0.4918\n",
            "Epoch 7 Batch 2200 Loss 1.0080 Accuracy 0.4920\n",
            "Epoch 7 Batch 2250 Loss 1.0049 Accuracy 0.4922\n",
            "Epoch 7 Batch 2300 Loss 1.0016 Accuracy 0.4924\n",
            "Epoch 7 Batch 2350 Loss 0.9983 Accuracy 0.4926\n",
            "Epoch 7 Batch 2400 Loss 0.9959 Accuracy 0.4928\n",
            "Epoch 7 Batch 2450 Loss 0.9933 Accuracy 0.4931\n",
            "Epoch 7 Batch 2500 Loss 0.9905 Accuracy 0.4934\n",
            "Epoch 7 Batch 2550 Loss 0.9874 Accuracy 0.4938\n",
            "Epoch 7 Batch 2600 Loss 0.9844 Accuracy 0.4942\n",
            "Epoch 7 Batch 2650 Loss 0.9820 Accuracy 0.4945\n",
            "Epoch 7 Batch 2700 Loss 0.9792 Accuracy 0.4949\n",
            "Epoch 7 Batch 2750 Loss 0.9772 Accuracy 0.4952\n",
            "Epoch 7 Batch 2800 Loss 0.9748 Accuracy 0.4954\n",
            "Epoch 7 Batch 2850 Loss 0.9729 Accuracy 0.4957\n",
            "Epoch 7 Batch 2900 Loss 0.9711 Accuracy 0.4960\n",
            "Epoch 7 Batch 2950 Loss 0.9692 Accuracy 0.4963\n",
            "Epoch 7 Batch 3000 Loss 0.9672 Accuracy 0.4966\n",
            "Epoch 7 Batch 3050 Loss 0.9656 Accuracy 0.4968\n",
            "Epoch 7 Batch 3100 Loss 0.9637 Accuracy 0.4971\n",
            "Epoch 7 Batch 3150 Loss 0.9621 Accuracy 0.4973\n",
            "Epoch 7 Batch 3200 Loss 0.9602 Accuracy 0.4975\n",
            "Epoch 7 Batch 3250 Loss 0.9584 Accuracy 0.4977\n",
            "Epoch 7 Batch 3300 Loss 0.9564 Accuracy 0.4980\n",
            "Epoch 7 Batch 3350 Loss 0.9544 Accuracy 0.4983\n",
            "Epoch 7 Batch 3400 Loss 0.9527 Accuracy 0.4986\n",
            "Epoch 7 Batch 3450 Loss 0.9505 Accuracy 0.4989\n",
            "Epoch 7 Batch 3500 Loss 0.9488 Accuracy 0.4993\n",
            "Epoch 7 Batch 3550 Loss 0.9473 Accuracy 0.4996\n",
            "Epoch 7 Batch 3600 Loss 0.9454 Accuracy 0.4999\n",
            "Epoch 7 Batch 3650 Loss 0.9438 Accuracy 0.5002\n",
            "Epoch 7 Batch 3700 Loss 0.9420 Accuracy 0.5005\n",
            "Epoch 7 Batch 3750 Loss 0.9402 Accuracy 0.5008\n",
            "Epoch 7 Batch 3800 Loss 0.9388 Accuracy 0.5011\n",
            "Epoch 7 Batch 3850 Loss 0.9374 Accuracy 0.5014\n",
            "Epoch 7 Batch 3900 Loss 0.9362 Accuracy 0.5017\n",
            "Epoch 7 Batch 3950 Loss 0.9350 Accuracy 0.5020\n",
            "Epoch 7 Batch 4000 Loss 0.9337 Accuracy 0.5024\n",
            "Epoch 7 Batch 4050 Loss 0.9323 Accuracy 0.5027\n",
            "Epoch 7 Batch 4100 Loss 0.9312 Accuracy 0.5029\n",
            "Epoch 7 Batch 4150 Loss 0.9306 Accuracy 0.5030\n",
            "Epoch 7 Batch 4200 Loss 0.9308 Accuracy 0.5030\n",
            "Epoch 7 Batch 4250 Loss 0.9315 Accuracy 0.5030\n",
            "Epoch 7 Batch 4300 Loss 0.9322 Accuracy 0.5029\n",
            "Epoch 7 Batch 4350 Loss 0.9332 Accuracy 0.5028\n",
            "Epoch 7 Batch 4400 Loss 0.9342 Accuracy 0.5026\n",
            "Epoch 7 Batch 4450 Loss 0.9354 Accuracy 0.5024\n",
            "Epoch 7 Batch 4500 Loss 0.9364 Accuracy 0.5023\n",
            "Epoch 7 Batch 4550 Loss 0.9374 Accuracy 0.5021\n",
            "Epoch 7 Batch 4600 Loss 0.9389 Accuracy 0.5019\n",
            "Epoch 7 Batch 4650 Loss 0.9404 Accuracy 0.5017\n",
            "Epoch 7 Batch 4700 Loss 0.9418 Accuracy 0.5015\n",
            "Epoch 7 Batch 4750 Loss 0.9431 Accuracy 0.5013\n",
            "Epoch 7 Batch 4800 Loss 0.9440 Accuracy 0.5011\n",
            "Epoch 7 Batch 4850 Loss 0.9454 Accuracy 0.5010\n",
            "Epoch 7 Batch 4900 Loss 0.9467 Accuracy 0.5009\n",
            "Epoch 7 Batch 4950 Loss 0.9478 Accuracy 0.5006\n",
            "Epoch 7 Batch 5000 Loss 0.9492 Accuracy 0.5004\n",
            "Epoch 7 Batch 5050 Loss 0.9505 Accuracy 0.5002\n",
            "Epoch 7 Batch 5100 Loss 0.9520 Accuracy 0.5000\n",
            "Epoch 7 Batch 5150 Loss 0.9531 Accuracy 0.4998\n",
            "Epoch 7 Batch 5200 Loss 0.9544 Accuracy 0.4995\n",
            "Epoch 7 Batch 5250 Loss 0.9554 Accuracy 0.4993\n",
            "Epoch 7 Batch 5300 Loss 0.9565 Accuracy 0.4990\n",
            "Epoch 7 Batch 5350 Loss 0.9575 Accuracy 0.4987\n",
            "Epoch 7 Batch 5400 Loss 0.9585 Accuracy 0.4984\n",
            "Epoch 7 Batch 5450 Loss 0.9595 Accuracy 0.4981\n",
            "Epoch 7 Batch 5500 Loss 0.9604 Accuracy 0.4978\n",
            "Epoch 7 Batch 5550 Loss 0.9614 Accuracy 0.4976\n",
            "Epoch 7 Batch 5600 Loss 0.9624 Accuracy 0.4974\n",
            "Epoch 7 Batch 5650 Loss 0.9633 Accuracy 0.4972\n",
            "Epoch 7 Batch 5700 Loss 0.9642 Accuracy 0.4969\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/projects/transformer/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 2095.081869840622 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.1926 Accuracy 0.4424\n",
            "Epoch 8 Batch 50 Loss 1.0466 Accuracy 0.4811\n",
            "Epoch 8 Batch 100 Loss 1.0543 Accuracy 0.4823\n",
            "Epoch 8 Batch 150 Loss 1.0617 Accuracy 0.4809\n",
            "Epoch 8 Batch 200 Loss 1.0604 Accuracy 0.4812\n",
            "Epoch 8 Batch 250 Loss 1.0667 Accuracy 0.4803\n",
            "Epoch 8 Batch 300 Loss 1.0600 Accuracy 0.4807\n",
            "Epoch 8 Batch 350 Loss 1.0568 Accuracy 0.4803\n",
            "Epoch 8 Batch 400 Loss 1.0531 Accuracy 0.4805\n",
            "Epoch 8 Batch 450 Loss 1.0480 Accuracy 0.4804\n",
            "Epoch 8 Batch 500 Loss 1.0456 Accuracy 0.4807\n",
            "Epoch 8 Batch 550 Loss 1.0451 Accuracy 0.4806\n",
            "Epoch 8 Batch 600 Loss 1.0440 Accuracy 0.4812\n",
            "Epoch 8 Batch 650 Loss 1.0445 Accuracy 0.4817\n",
            "Epoch 8 Batch 700 Loss 1.0445 Accuracy 0.4821\n",
            "Epoch 8 Batch 750 Loss 1.0436 Accuracy 0.4820\n",
            "Epoch 8 Batch 800 Loss 1.0429 Accuracy 0.4824\n",
            "Epoch 8 Batch 850 Loss 1.0429 Accuracy 0.4821\n",
            "Epoch 8 Batch 900 Loss 1.0417 Accuracy 0.4820\n",
            "Epoch 8 Batch 950 Loss 1.0398 Accuracy 0.4822\n",
            "Epoch 8 Batch 1000 Loss 1.0393 Accuracy 0.4822\n",
            "Epoch 8 Batch 1050 Loss 1.0377 Accuracy 0.4824\n",
            "Epoch 8 Batch 1100 Loss 1.0369 Accuracy 0.4825\n",
            "Epoch 8 Batch 1150 Loss 1.0351 Accuracy 0.4827\n",
            "Epoch 8 Batch 1200 Loss 1.0337 Accuracy 0.4830\n",
            "Epoch 8 Batch 1250 Loss 1.0322 Accuracy 0.4834\n",
            "Epoch 8 Batch 1300 Loss 1.0298 Accuracy 0.4838\n",
            "Epoch 8 Batch 1350 Loss 1.0268 Accuracy 0.4845\n",
            "Epoch 8 Batch 1400 Loss 1.0241 Accuracy 0.4853\n",
            "Epoch 8 Batch 1450 Loss 1.0212 Accuracy 0.4860\n",
            "Epoch 8 Batch 1500 Loss 1.0192 Accuracy 0.4866\n",
            "Epoch 8 Batch 1550 Loss 1.0166 Accuracy 0.4876\n",
            "Epoch 8 Batch 1600 Loss 1.0133 Accuracy 0.4885\n",
            "Epoch 8 Batch 1650 Loss 1.0112 Accuracy 0.4894\n",
            "Epoch 8 Batch 1700 Loss 1.0093 Accuracy 0.4903\n",
            "Epoch 8 Batch 1750 Loss 1.0072 Accuracy 0.4911\n",
            "Epoch 8 Batch 1800 Loss 1.0050 Accuracy 0.4917\n",
            "Epoch 8 Batch 1850 Loss 1.0031 Accuracy 0.4926\n",
            "Epoch 8 Batch 1900 Loss 1.0005 Accuracy 0.4932\n",
            "Epoch 8 Batch 1950 Loss 0.9984 Accuracy 0.4939\n",
            "Epoch 8 Batch 2000 Loss 0.9964 Accuracy 0.4944\n",
            "Epoch 8 Batch 2050 Loss 0.9944 Accuracy 0.4949\n",
            "Epoch 8 Batch 2100 Loss 0.9920 Accuracy 0.4953\n",
            "Epoch 8 Batch 2150 Loss 0.9889 Accuracy 0.4956\n",
            "Epoch 8 Batch 2200 Loss 0.9855 Accuracy 0.4958\n",
            "Epoch 8 Batch 2250 Loss 0.9826 Accuracy 0.4961\n",
            "Epoch 8 Batch 2300 Loss 0.9798 Accuracy 0.4963\n",
            "Epoch 8 Batch 2350 Loss 0.9771 Accuracy 0.4964\n",
            "Epoch 8 Batch 2400 Loss 0.9740 Accuracy 0.4966\n",
            "Epoch 8 Batch 2450 Loss 0.9716 Accuracy 0.4969\n",
            "Epoch 8 Batch 2500 Loss 0.9688 Accuracy 0.4971\n",
            "Epoch 8 Batch 2550 Loss 0.9660 Accuracy 0.4975\n",
            "Epoch 8 Batch 2600 Loss 0.9632 Accuracy 0.4979\n",
            "Epoch 8 Batch 2650 Loss 0.9608 Accuracy 0.4982\n",
            "Epoch 8 Batch 2700 Loss 0.9583 Accuracy 0.4986\n",
            "Epoch 8 Batch 2750 Loss 0.9559 Accuracy 0.4988\n",
            "Epoch 8 Batch 2800 Loss 0.9536 Accuracy 0.4991\n",
            "Epoch 8 Batch 2850 Loss 0.9515 Accuracy 0.4994\n",
            "Epoch 8 Batch 2900 Loss 0.9495 Accuracy 0.4997\n",
            "Epoch 8 Batch 2950 Loss 0.9476 Accuracy 0.4999\n",
            "Epoch 8 Batch 3000 Loss 0.9460 Accuracy 0.5002\n",
            "Epoch 8 Batch 3050 Loss 0.9444 Accuracy 0.5005\n",
            "Epoch 8 Batch 3100 Loss 0.9423 Accuracy 0.5007\n",
            "Epoch 8 Batch 3150 Loss 0.9404 Accuracy 0.5009\n",
            "Epoch 8 Batch 3200 Loss 0.9386 Accuracy 0.5012\n",
            "Epoch 8 Batch 3250 Loss 0.9367 Accuracy 0.5014\n",
            "Epoch 8 Batch 3300 Loss 0.9347 Accuracy 0.5017\n",
            "Epoch 8 Batch 3350 Loss 0.9331 Accuracy 0.5020\n",
            "Epoch 8 Batch 3400 Loss 0.9309 Accuracy 0.5022\n",
            "Epoch 8 Batch 3450 Loss 0.9291 Accuracy 0.5025\n",
            "Epoch 8 Batch 3500 Loss 0.9274 Accuracy 0.5029\n",
            "Epoch 8 Batch 3550 Loss 0.9256 Accuracy 0.5032\n",
            "Epoch 8 Batch 3600 Loss 0.9236 Accuracy 0.5035\n",
            "Epoch 8 Batch 3650 Loss 0.9219 Accuracy 0.5039\n",
            "Epoch 8 Batch 3700 Loss 0.9202 Accuracy 0.5043\n",
            "Epoch 8 Batch 3750 Loss 0.9186 Accuracy 0.5046\n",
            "Epoch 8 Batch 3800 Loss 0.9170 Accuracy 0.5049\n",
            "Epoch 8 Batch 3850 Loss 0.9159 Accuracy 0.5053\n",
            "Epoch 8 Batch 3900 Loss 0.9143 Accuracy 0.5055\n",
            "Epoch 8 Batch 3950 Loss 0.9128 Accuracy 0.5058\n",
            "Epoch 8 Batch 4000 Loss 0.9113 Accuracy 0.5061\n",
            "Epoch 8 Batch 4050 Loss 0.9103 Accuracy 0.5064\n",
            "Epoch 8 Batch 4100 Loss 0.9092 Accuracy 0.5066\n",
            "Epoch 8 Batch 4150 Loss 0.9088 Accuracy 0.5067\n",
            "Epoch 8 Batch 4200 Loss 0.9090 Accuracy 0.5068\n",
            "Epoch 8 Batch 4250 Loss 0.9095 Accuracy 0.5067\n",
            "Epoch 8 Batch 4300 Loss 0.9105 Accuracy 0.5065\n",
            "Epoch 8 Batch 4350 Loss 0.9115 Accuracy 0.5064\n",
            "Epoch 8 Batch 4400 Loss 0.9125 Accuracy 0.5063\n",
            "Epoch 8 Batch 4450 Loss 0.9136 Accuracy 0.5062\n",
            "Epoch 8 Batch 4500 Loss 0.9147 Accuracy 0.5060\n",
            "Epoch 8 Batch 4550 Loss 0.9161 Accuracy 0.5057\n",
            "Epoch 8 Batch 4600 Loss 0.9175 Accuracy 0.5055\n",
            "Epoch 8 Batch 4650 Loss 0.9190 Accuracy 0.5053\n",
            "Epoch 8 Batch 4700 Loss 0.9202 Accuracy 0.5051\n",
            "Epoch 8 Batch 4750 Loss 0.9216 Accuracy 0.5049\n",
            "Epoch 8 Batch 4800 Loss 0.9230 Accuracy 0.5047\n",
            "Epoch 8 Batch 4850 Loss 0.9242 Accuracy 0.5046\n",
            "Epoch 8 Batch 4900 Loss 0.9256 Accuracy 0.5044\n",
            "Epoch 8 Batch 4950 Loss 0.9269 Accuracy 0.5042\n",
            "Epoch 8 Batch 5000 Loss 0.9281 Accuracy 0.5040\n",
            "Epoch 8 Batch 5050 Loss 0.9293 Accuracy 0.5038\n",
            "Epoch 8 Batch 5100 Loss 0.9306 Accuracy 0.5036\n",
            "Epoch 8 Batch 5150 Loss 0.9319 Accuracy 0.5033\n",
            "Epoch 8 Batch 5200 Loss 0.9334 Accuracy 0.5030\n",
            "Epoch 8 Batch 5250 Loss 0.9345 Accuracy 0.5027\n",
            "Epoch 8 Batch 5300 Loss 0.9356 Accuracy 0.5024\n",
            "Epoch 8 Batch 5350 Loss 0.9367 Accuracy 0.5022\n",
            "Epoch 8 Batch 5400 Loss 0.9378 Accuracy 0.5019\n",
            "Epoch 8 Batch 5450 Loss 0.9388 Accuracy 0.5016\n",
            "Epoch 8 Batch 5500 Loss 0.9398 Accuracy 0.5014\n",
            "Epoch 8 Batch 5550 Loss 0.9409 Accuracy 0.5011\n",
            "Epoch 8 Batch 5600 Loss 0.9418 Accuracy 0.5009\n",
            "Epoch 8 Batch 5650 Loss 0.9427 Accuracy 0.5006\n",
            "Epoch 8 Batch 5700 Loss 0.9436 Accuracy 0.5004\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/projects/transformer/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 2152.1770136356354 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 1.1541 Accuracy 0.4926\n",
            "Epoch 9 Batch 50 Loss 1.0497 Accuracy 0.4877\n",
            "Epoch 9 Batch 100 Loss 1.0476 Accuracy 0.4837\n",
            "Epoch 9 Batch 150 Loss 1.0440 Accuracy 0.4837\n",
            "Epoch 9 Batch 200 Loss 1.0450 Accuracy 0.4847\n",
            "Epoch 9 Batch 250 Loss 1.0451 Accuracy 0.4851\n",
            "Epoch 9 Batch 300 Loss 1.0429 Accuracy 0.4848\n",
            "Epoch 9 Batch 350 Loss 1.0382 Accuracy 0.4848\n",
            "Epoch 9 Batch 400 Loss 1.0323 Accuracy 0.4852\n",
            "Epoch 9 Batch 450 Loss 1.0298 Accuracy 0.4847\n",
            "Epoch 9 Batch 500 Loss 1.0259 Accuracy 0.4849\n",
            "Epoch 9 Batch 550 Loss 1.0271 Accuracy 0.4849\n",
            "Epoch 9 Batch 600 Loss 1.0265 Accuracy 0.4850\n",
            "Epoch 9 Batch 650 Loss 1.0252 Accuracy 0.4855\n",
            "Epoch 9 Batch 700 Loss 1.0241 Accuracy 0.4860\n",
            "Epoch 9 Batch 750 Loss 1.0225 Accuracy 0.4861\n",
            "Epoch 9 Batch 800 Loss 1.0232 Accuracy 0.4860\n",
            "Epoch 9 Batch 850 Loss 1.0233 Accuracy 0.4862\n",
            "Epoch 9 Batch 900 Loss 1.0222 Accuracy 0.4860\n",
            "Epoch 9 Batch 950 Loss 1.0199 Accuracy 0.4860\n",
            "Epoch 9 Batch 1000 Loss 1.0179 Accuracy 0.4858\n",
            "Epoch 9 Batch 1050 Loss 1.0179 Accuracy 0.4857\n",
            "Epoch 9 Batch 1100 Loss 1.0163 Accuracy 0.4860\n",
            "Epoch 9 Batch 1150 Loss 1.0154 Accuracy 0.4864\n",
            "Epoch 9 Batch 1200 Loss 1.0137 Accuracy 0.4863\n",
            "Epoch 9 Batch 1250 Loss 1.0126 Accuracy 0.4868\n",
            "Epoch 9 Batch 1300 Loss 1.0100 Accuracy 0.4874\n",
            "Epoch 9 Batch 1350 Loss 1.0080 Accuracy 0.4878\n",
            "Epoch 9 Batch 1400 Loss 1.0053 Accuracy 0.4885\n",
            "Epoch 9 Batch 1450 Loss 1.0027 Accuracy 0.4893\n",
            "Epoch 9 Batch 1500 Loss 0.9998 Accuracy 0.4901\n",
            "Epoch 9 Batch 1550 Loss 0.9969 Accuracy 0.4910\n",
            "Epoch 9 Batch 1600 Loss 0.9945 Accuracy 0.4920\n",
            "Epoch 9 Batch 1650 Loss 0.9927 Accuracy 0.4928\n",
            "Epoch 9 Batch 1700 Loss 0.9899 Accuracy 0.4937\n",
            "Epoch 9 Batch 1750 Loss 0.9874 Accuracy 0.4945\n",
            "Epoch 9 Batch 1800 Loss 0.9850 Accuracy 0.4952\n",
            "Epoch 9 Batch 1850 Loss 0.9828 Accuracy 0.4958\n",
            "Epoch 9 Batch 1900 Loss 0.9802 Accuracy 0.4965\n",
            "Epoch 9 Batch 1950 Loss 0.9783 Accuracy 0.4973\n",
            "Epoch 9 Batch 2000 Loss 0.9762 Accuracy 0.4979\n",
            "Epoch 9 Batch 2050 Loss 0.9747 Accuracy 0.4983\n",
            "Epoch 9 Batch 2100 Loss 0.9722 Accuracy 0.4986\n",
            "Epoch 9 Batch 2150 Loss 0.9696 Accuracy 0.4989\n",
            "Epoch 9 Batch 2200 Loss 0.9662 Accuracy 0.4991\n",
            "Epoch 9 Batch 2250 Loss 0.9635 Accuracy 0.4993\n",
            "Epoch 9 Batch 2300 Loss 0.9607 Accuracy 0.4996\n",
            "Epoch 9 Batch 2350 Loss 0.9579 Accuracy 0.4998\n",
            "Epoch 9 Batch 2400 Loss 0.9550 Accuracy 0.5000\n",
            "Epoch 9 Batch 2450 Loss 0.9520 Accuracy 0.5003\n",
            "Epoch 9 Batch 2500 Loss 0.9496 Accuracy 0.5006\n",
            "Epoch 9 Batch 2550 Loss 0.9466 Accuracy 0.5009\n",
            "Epoch 9 Batch 2600 Loss 0.9436 Accuracy 0.5013\n",
            "Epoch 9 Batch 2650 Loss 0.9412 Accuracy 0.5017\n",
            "Epoch 9 Batch 2700 Loss 0.9387 Accuracy 0.5019\n",
            "Epoch 9 Batch 2750 Loss 0.9359 Accuracy 0.5021\n",
            "Epoch 9 Batch 2800 Loss 0.9342 Accuracy 0.5025\n",
            "Epoch 9 Batch 2850 Loss 0.9320 Accuracy 0.5028\n",
            "Epoch 9 Batch 2900 Loss 0.9300 Accuracy 0.5031\n",
            "Epoch 9 Batch 2950 Loss 0.9281 Accuracy 0.5033\n",
            "Epoch 9 Batch 3000 Loss 0.9263 Accuracy 0.5035\n",
            "Epoch 9 Batch 3050 Loss 0.9244 Accuracy 0.5037\n",
            "Epoch 9 Batch 3100 Loss 0.9226 Accuracy 0.5040\n",
            "Epoch 9 Batch 3150 Loss 0.9212 Accuracy 0.5043\n",
            "Epoch 9 Batch 3200 Loss 0.9192 Accuracy 0.5045\n",
            "Epoch 9 Batch 3250 Loss 0.9175 Accuracy 0.5047\n",
            "Epoch 9 Batch 3300 Loss 0.9154 Accuracy 0.5050\n",
            "Epoch 9 Batch 3350 Loss 0.9135 Accuracy 0.5053\n",
            "Epoch 9 Batch 3400 Loss 0.9120 Accuracy 0.5056\n",
            "Epoch 9 Batch 3450 Loss 0.9101 Accuracy 0.5059\n",
            "Epoch 9 Batch 3500 Loss 0.9088 Accuracy 0.5061\n",
            "Epoch 9 Batch 3550 Loss 0.9070 Accuracy 0.5065\n",
            "Epoch 9 Batch 3600 Loss 0.9054 Accuracy 0.5067\n",
            "Epoch 9 Batch 3650 Loss 0.9037 Accuracy 0.5071\n",
            "Epoch 9 Batch 3700 Loss 0.9019 Accuracy 0.5073\n",
            "Epoch 9 Batch 3750 Loss 0.9005 Accuracy 0.5076\n",
            "Epoch 9 Batch 3800 Loss 0.8990 Accuracy 0.5079\n",
            "Epoch 9 Batch 3850 Loss 0.8975 Accuracy 0.5083\n",
            "Epoch 9 Batch 3900 Loss 0.8962 Accuracy 0.5085\n",
            "Epoch 9 Batch 3950 Loss 0.8948 Accuracy 0.5089\n",
            "Epoch 9 Batch 4000 Loss 0.8934 Accuracy 0.5092\n",
            "Epoch 9 Batch 4050 Loss 0.8922 Accuracy 0.5096\n",
            "Epoch 9 Batch 4100 Loss 0.8912 Accuracy 0.5097\n",
            "Epoch 9 Batch 4150 Loss 0.8910 Accuracy 0.5099\n",
            "Epoch 9 Batch 4200 Loss 0.8912 Accuracy 0.5098\n",
            "Epoch 9 Batch 4250 Loss 0.8912 Accuracy 0.5098\n",
            "Epoch 9 Batch 4300 Loss 0.8919 Accuracy 0.5097\n",
            "Epoch 9 Batch 4350 Loss 0.8927 Accuracy 0.5096\n",
            "Epoch 9 Batch 4400 Loss 0.8938 Accuracy 0.5094\n",
            "Epoch 9 Batch 4450 Loss 0.8949 Accuracy 0.5092\n",
            "Epoch 9 Batch 4500 Loss 0.8962 Accuracy 0.5090\n",
            "Epoch 9 Batch 4550 Loss 0.8975 Accuracy 0.5087\n",
            "Epoch 9 Batch 4600 Loss 0.8990 Accuracy 0.5086\n",
            "Epoch 9 Batch 4650 Loss 0.9004 Accuracy 0.5084\n",
            "Epoch 9 Batch 4700 Loss 0.9021 Accuracy 0.5082\n",
            "Epoch 9 Batch 4750 Loss 0.9035 Accuracy 0.5080\n",
            "Epoch 9 Batch 4800 Loss 0.9046 Accuracy 0.5078\n",
            "Epoch 9 Batch 4850 Loss 0.9058 Accuracy 0.5076\n",
            "Epoch 9 Batch 4900 Loss 0.9071 Accuracy 0.5074\n",
            "Epoch 9 Batch 4950 Loss 0.9082 Accuracy 0.5072\n",
            "Epoch 9 Batch 5000 Loss 0.9095 Accuracy 0.5071\n",
            "Epoch 9 Batch 5050 Loss 0.9112 Accuracy 0.5068\n",
            "Epoch 9 Batch 5100 Loss 0.9126 Accuracy 0.5066\n",
            "Epoch 9 Batch 5150 Loss 0.9139 Accuracy 0.5064\n",
            "Epoch 9 Batch 5200 Loss 0.9150 Accuracy 0.5061\n",
            "Epoch 9 Batch 5250 Loss 0.9161 Accuracy 0.5058\n",
            "Epoch 9 Batch 5300 Loss 0.9173 Accuracy 0.5056\n",
            "Epoch 9 Batch 5350 Loss 0.9185 Accuracy 0.5052\n",
            "Epoch 9 Batch 5400 Loss 0.9195 Accuracy 0.5050\n",
            "Epoch 9 Batch 5450 Loss 0.9205 Accuracy 0.5047\n",
            "Epoch 9 Batch 5500 Loss 0.9215 Accuracy 0.5044\n",
            "Epoch 9 Batch 5550 Loss 0.9227 Accuracy 0.5042\n",
            "Epoch 9 Batch 5600 Loss 0.9237 Accuracy 0.5039\n",
            "Epoch 9 Batch 5650 Loss 0.9247 Accuracy 0.5037\n",
            "Epoch 9 Batch 5700 Loss 0.9255 Accuracy 0.5034\n",
            "Saving checkpoint for epoch 9 at ./drive/My Drive/projects/transformer/ckpt/ckpt-9\n",
            "Time taken for 1 epoch: 2145.7857065200806 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.0340 Accuracy 0.4737\n",
            "Epoch 10 Batch 50 Loss 1.0344 Accuracy 0.4863\n",
            "Epoch 10 Batch 100 Loss 1.0209 Accuracy 0.4876\n",
            "Epoch 10 Batch 150 Loss 1.0152 Accuracy 0.4879\n",
            "Epoch 10 Batch 200 Loss 1.0203 Accuracy 0.4866\n",
            "Epoch 10 Batch 250 Loss 1.0188 Accuracy 0.4863\n",
            "Epoch 10 Batch 300 Loss 1.0199 Accuracy 0.4865\n",
            "Epoch 10 Batch 350 Loss 1.0213 Accuracy 0.4868\n",
            "Epoch 10 Batch 400 Loss 1.0180 Accuracy 0.4872\n",
            "Epoch 10 Batch 450 Loss 1.0138 Accuracy 0.4870\n",
            "Epoch 10 Batch 500 Loss 1.0123 Accuracy 0.4875\n",
            "Epoch 10 Batch 550 Loss 1.0104 Accuracy 0.4873\n",
            "Epoch 10 Batch 600 Loss 1.0095 Accuracy 0.4872\n",
            "Epoch 10 Batch 650 Loss 1.0092 Accuracy 0.4877\n",
            "Epoch 10 Batch 700 Loss 1.0088 Accuracy 0.4879\n",
            "Epoch 10 Batch 750 Loss 1.0082 Accuracy 0.4883\n",
            "Epoch 10 Batch 800 Loss 1.0069 Accuracy 0.4886\n",
            "Epoch 10 Batch 850 Loss 1.0064 Accuracy 0.4886\n",
            "Epoch 10 Batch 900 Loss 1.0062 Accuracy 0.4887\n",
            "Epoch 10 Batch 950 Loss 1.0045 Accuracy 0.4885\n",
            "Epoch 10 Batch 1000 Loss 1.0024 Accuracy 0.4885\n",
            "Epoch 10 Batch 1050 Loss 1.0019 Accuracy 0.4884\n",
            "Epoch 10 Batch 1100 Loss 1.0014 Accuracy 0.4884\n",
            "Epoch 10 Batch 1150 Loss 0.9995 Accuracy 0.4886\n",
            "Epoch 10 Batch 1200 Loss 0.9982 Accuracy 0.4888\n",
            "Epoch 10 Batch 1250 Loss 0.9963 Accuracy 0.4893\n",
            "Epoch 10 Batch 1300 Loss 0.9942 Accuracy 0.4899\n",
            "Epoch 10 Batch 1350 Loss 0.9918 Accuracy 0.4904\n",
            "Epoch 10 Batch 1400 Loss 0.9892 Accuracy 0.4911\n",
            "Epoch 10 Batch 1450 Loss 0.9868 Accuracy 0.4918\n",
            "Epoch 10 Batch 1500 Loss 0.9842 Accuracy 0.4925\n",
            "Epoch 10 Batch 1550 Loss 0.9817 Accuracy 0.4933\n",
            "Epoch 10 Batch 1600 Loss 0.9786 Accuracy 0.4940\n",
            "Epoch 10 Batch 1650 Loss 0.9764 Accuracy 0.4947\n",
            "Epoch 10 Batch 1700 Loss 0.9740 Accuracy 0.4954\n",
            "Epoch 10 Batch 1750 Loss 0.9723 Accuracy 0.4963\n",
            "Epoch 10 Batch 1800 Loss 0.9704 Accuracy 0.4972\n",
            "Epoch 10 Batch 1850 Loss 0.9683 Accuracy 0.4980\n",
            "Epoch 10 Batch 1900 Loss 0.9653 Accuracy 0.4989\n",
            "Epoch 10 Batch 1950 Loss 0.9635 Accuracy 0.4995\n",
            "Epoch 10 Batch 2000 Loss 0.9618 Accuracy 0.5000\n",
            "Epoch 10 Batch 2050 Loss 0.9598 Accuracy 0.5005\n",
            "Epoch 10 Batch 2100 Loss 0.9574 Accuracy 0.5009\n",
            "Epoch 10 Batch 2150 Loss 0.9546 Accuracy 0.5013\n",
            "Epoch 10 Batch 2200 Loss 0.9518 Accuracy 0.5015\n",
            "Epoch 10 Batch 2250 Loss 0.9489 Accuracy 0.5017\n",
            "Epoch 10 Batch 2300 Loss 0.9456 Accuracy 0.5019\n",
            "Epoch 10 Batch 2350 Loss 0.9430 Accuracy 0.5021\n",
            "Epoch 10 Batch 2400 Loss 0.9403 Accuracy 0.5022\n",
            "Epoch 10 Batch 2450 Loss 0.9377 Accuracy 0.5025\n",
            "Epoch 10 Batch 2500 Loss 0.9347 Accuracy 0.5028\n",
            "Epoch 10 Batch 2550 Loss 0.9323 Accuracy 0.5033\n",
            "Epoch 10 Batch 2600 Loss 0.9295 Accuracy 0.5036\n",
            "Epoch 10 Batch 2650 Loss 0.9271 Accuracy 0.5040\n",
            "Epoch 10 Batch 2700 Loss 0.9246 Accuracy 0.5042\n",
            "Epoch 10 Batch 2750 Loss 0.9224 Accuracy 0.5045\n",
            "Epoch 10 Batch 2800 Loss 0.9201 Accuracy 0.5049\n",
            "Epoch 10 Batch 2850 Loss 0.9179 Accuracy 0.5051\n",
            "Epoch 10 Batch 2900 Loss 0.9161 Accuracy 0.5055\n",
            "Epoch 10 Batch 2950 Loss 0.9140 Accuracy 0.5057\n",
            "Epoch 10 Batch 3000 Loss 0.9121 Accuracy 0.5059\n",
            "Epoch 10 Batch 3050 Loss 0.9105 Accuracy 0.5062\n",
            "Epoch 10 Batch 3100 Loss 0.9084 Accuracy 0.5065\n",
            "Epoch 10 Batch 3150 Loss 0.9061 Accuracy 0.5067\n",
            "Epoch 10 Batch 3200 Loss 0.9043 Accuracy 0.5068\n",
            "Epoch 10 Batch 3250 Loss 0.9024 Accuracy 0.5071\n",
            "Epoch 10 Batch 3300 Loss 0.9003 Accuracy 0.5073\n",
            "Epoch 10 Batch 3350 Loss 0.8986 Accuracy 0.5076\n",
            "Epoch 10 Batch 3400 Loss 0.8972 Accuracy 0.5079\n",
            "Epoch 10 Batch 3450 Loss 0.8953 Accuracy 0.5081\n",
            "Epoch 10 Batch 3500 Loss 0.8936 Accuracy 0.5085\n",
            "Epoch 10 Batch 3550 Loss 0.8918 Accuracy 0.5087\n",
            "Epoch 10 Batch 3600 Loss 0.8899 Accuracy 0.5089\n",
            "Epoch 10 Batch 3650 Loss 0.8885 Accuracy 0.5093\n",
            "Epoch 10 Batch 3700 Loss 0.8870 Accuracy 0.5095\n",
            "Epoch 10 Batch 3750 Loss 0.8855 Accuracy 0.5099\n",
            "Epoch 10 Batch 3800 Loss 0.8841 Accuracy 0.5104\n",
            "Epoch 10 Batch 3850 Loss 0.8828 Accuracy 0.5107\n",
            "Epoch 10 Batch 3900 Loss 0.8813 Accuracy 0.5109\n",
            "Epoch 10 Batch 3950 Loss 0.8799 Accuracy 0.5112\n",
            "Epoch 10 Batch 4000 Loss 0.8786 Accuracy 0.5115\n",
            "Epoch 10 Batch 4050 Loss 0.8773 Accuracy 0.5118\n",
            "Epoch 10 Batch 4100 Loss 0.8765 Accuracy 0.5119\n",
            "Epoch 10 Batch 4150 Loss 0.8761 Accuracy 0.5120\n",
            "Epoch 10 Batch 4200 Loss 0.8763 Accuracy 0.5120\n",
            "Epoch 10 Batch 4250 Loss 0.8769 Accuracy 0.5120\n",
            "Epoch 10 Batch 4300 Loss 0.8774 Accuracy 0.5120\n",
            "Epoch 10 Batch 4350 Loss 0.8783 Accuracy 0.5118\n",
            "Epoch 10 Batch 4400 Loss 0.8797 Accuracy 0.5116\n",
            "Epoch 10 Batch 4450 Loss 0.8810 Accuracy 0.5114\n",
            "Epoch 10 Batch 4500 Loss 0.8821 Accuracy 0.5112\n",
            "Epoch 10 Batch 4550 Loss 0.8835 Accuracy 0.5110\n",
            "Epoch 10 Batch 4600 Loss 0.8848 Accuracy 0.5108\n",
            "Epoch 10 Batch 4650 Loss 0.8864 Accuracy 0.5107\n",
            "Epoch 10 Batch 4700 Loss 0.8878 Accuracy 0.5105\n",
            "Epoch 10 Batch 4750 Loss 0.8890 Accuracy 0.5103\n",
            "Epoch 10 Batch 4800 Loss 0.8901 Accuracy 0.5101\n",
            "Epoch 10 Batch 4850 Loss 0.8913 Accuracy 0.5099\n",
            "Epoch 10 Batch 4900 Loss 0.8925 Accuracy 0.5098\n",
            "Epoch 10 Batch 4950 Loss 0.8938 Accuracy 0.5096\n",
            "Epoch 10 Batch 5000 Loss 0.8952 Accuracy 0.5094\n",
            "Epoch 10 Batch 5050 Loss 0.8968 Accuracy 0.5091\n",
            "Epoch 10 Batch 5100 Loss 0.8981 Accuracy 0.5089\n",
            "Epoch 10 Batch 5150 Loss 0.8993 Accuracy 0.5087\n",
            "Epoch 10 Batch 5200 Loss 0.9005 Accuracy 0.5084\n",
            "Epoch 10 Batch 5250 Loss 0.9018 Accuracy 0.5081\n",
            "Epoch 10 Batch 5300 Loss 0.9030 Accuracy 0.5077\n",
            "Epoch 10 Batch 5350 Loss 0.9042 Accuracy 0.5075\n",
            "Epoch 10 Batch 5400 Loss 0.9051 Accuracy 0.5073\n",
            "Epoch 10 Batch 5450 Loss 0.9061 Accuracy 0.5070\n",
            "Epoch 10 Batch 5500 Loss 0.9070 Accuracy 0.5068\n",
            "Epoch 10 Batch 5550 Loss 0.9080 Accuracy 0.5065\n",
            "Epoch 10 Batch 5600 Loss 0.9089 Accuracy 0.5063\n",
            "Epoch 10 Batch 5650 Loss 0.9100 Accuracy 0.5060\n",
            "Epoch 10 Batch 5700 Loss 0.9111 Accuracy 0.5057\n",
            "Saving checkpoint for epoch 10 at ./drive/My Drive/projects/transformer/ckpt/ckpt-10\n",
            "Time taken for 1 epoch: 2133.53227686882 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzyRwDrRGdq",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNHwJJrz3lPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6VeFKrE6Kdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdoWKbCP7Czs",
        "colab_type": "code",
        "outputId": "f9753912-f39e-4a53-9ab2-0c2acdf10c2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: This is a really powerful tool!\n",
            "Predicted translation: C'est un instrument vraiment puissant!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWll7JnTgB_X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0cc9ed03-615f-4086-c6c6-22b629a98e15"
      },
      "source": [
        "translate(\"I am happy !\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: I am happy !\n",
            "Predicted translation: Je suis content!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1THKhqZmgOLx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "06d52e80-0a3e-4b88-d3a9-8762eb6c6e8c"
      },
      "source": [
        "translate('What do you think of my work?')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: What do you think of my work?\n",
            "Predicted translation: Que pensez-vous de mon travail ?\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}